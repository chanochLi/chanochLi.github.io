<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Jetson+gemini 2相机配置教程 | Chanoch的博客</title><meta name="author" content="Chanoch Li"><meta name="copyright" content="Chanoch Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Jetson平台和Gemini 2相机平台Jetson Orin NX&#x2F;Jetson AGX OrinNVIDIA Jetson™ 是专为机器人和嵌入式边缘 AI 应用打造的平台，设计紧凑但功能强大的计算机，并由NVIDIA JetPack™ SDK提供支持，能够加速软件开发。 产品次代：原始系列-&gt;Xavier系列-&gt;Orin系列-&gt;Thor系列(未来推出) 等级：N"><meta property="og:type" content="article"><meta property="og:title" content="Jetson+gemini 2相机配置教程"><meta property="og:url" content="https://blog.chanoch.top/2025/05/04/Jetson%E6%95%99%E7%A8%8B/index.html"><meta property="og:site_name" content="Chanoch的博客"><meta property="og:description" content="Jetson平台和Gemini 2相机平台Jetson Orin NX&#x2F;Jetson AGX OrinNVIDIA Jetson™ 是专为机器人和嵌入式边缘 AI 应用打造的平台，设计紧凑但功能强大的计算机，并由NVIDIA JetPack™ SDK提供支持，能够加速软件开发。 产品次代：原始系列-&gt;Xavier系列-&gt;Orin系列-&gt;Thor系列(未来推出) 等级：N"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog.chanoch.top/img/avatar.png"><meta property="article:published_time" content="2025-05-04T04:22:39.000Z"><meta property="article:modified_time" content="2025-07-19T11:20:30.925Z"><meta property="article:author" content="Chanoch Li"><meta property="article:tag" content="Jetson"><meta property="article:tag" content="边缘计算"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.chanoch.top/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Jetson+gemini 2相机配置教程",
  "url": "https://blog.chanoch.top/2025/05/04/Jetson%E6%95%99%E7%A8%8B/",
  "image": "https://blog.chanoch.top/img/avatar.png",
  "datePublished": "2025-05-04T04:22:39.000Z",
  "dateModified": "2025-07-19T11:20:30.925Z",
  "author": [
    {
      "@type": "Person",
      "name": "Chanoch Li",
      "url": "https://blog.chanoch.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="https://blog.chanoch.top/2025/05/04/Jetson%E6%95%99%E7%A8%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Jetson+gemini 2相机配置教程",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Chanoch的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">Jetson+gemini 2相机配置教程</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Jetson+gemini 2相机配置教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-04T04:22:39.000Z" title="发表于 2025-05-04 12:22:39">2025-05-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-19T11:20:30.925Z" title="更新于 2025-07-19 19:20:30">2025-07-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Jetson/">Jetson</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:30,&quot;messagePrev&quot;:&quot;自从上次更新，已经过了&quot;,&quot;messageNext&quot;:&quot;天，文章内容可能已经过时。&quot;,&quot;postUpdate&quot;:&quot;2025-07-19 19:20:30&quot;}" hidden></div><h1 id="Jetson平台和Gemini-2相机平台"><a href="#Jetson平台和Gemini-2相机平台" class="headerlink" title="Jetson平台和Gemini 2相机平台"></a>Jetson平台和Gemini 2相机平台</h1><h3 id="Jetson-Orin-NX-Jetson-AGX-Orin"><a href="#Jetson-Orin-NX-Jetson-AGX-Orin" class="headerlink" title="Jetson Orin NX&#x2F;Jetson AGX Orin"></a>Jetson Orin NX&#x2F;Jetson AGX Orin</h3><p>NVIDIA Jetson™ 是专为机器人和嵌入式边缘 AI 应用打造的平台，设计紧凑但功能强大的<strong>计算机</strong>，并由NVIDIA JetPack™ SDK提供支持，能够加速软件开发。</p><p>产品次代：原始系列-&gt;Xavier系列-&gt;Orin系列-&gt;Thor系列(未来推出)</p><p>等级：Nano，NX，AGX</p><blockquote><p>完整规格表：<a target="_blank" rel="noopener" href="https://www.nvidia.cn/autonomous-machines/embedded-systems/#jetson-compare">https://www.nvidia.cn/autonomous-machines/embedded-systems/#jetson-compare</a></p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250719122100263.png" alt="image-20250719122100263"></p><ul><li><p>AI性能：可以横向对比在GPU上运行程序的速度，例如目标检测，分类等，但值得注意的是，这里是指理想情况下(最大功耗，合适温度条件下)的最大性能(INT8精度)</p></li><li><p>CPU：可以横向对比在CPU上运行程序的速度，例如图像采集，流程控制等</p></li><li><p>显存：与传统计算机不同，Jetson的内存类似于手机，CPU和GPU共用显存，显存决定了能跑多大的模型，跑多少程序等</p></li><li><p>存储：可以安装系统，存储文件等等，但在实际使用中，eMMC存储通常只用于调试，不推荐使用eMMC存储在生产环境中(容易)，强烈建议增加NVMe存储</p></li></ul><p>选购建议：可以根据显存进行选择，可以查询所使用的模型需要多少显存运行，在此基础上+4G(用于系统运行控制等)显存的规格就可以</p><blockquote><p>一般来说，只要任务不是太离谱，速度都可以通过软件优化满足，当然，花钱解决也是一种途径</p><p>越大规模的功耗一般更高，可能会不满足使用条件，这也是要考虑的一点，由此选择越小的会越稳定</p></blockquote><h3 id="Gemini-2"><a href="#Gemini-2" class="headerlink" title="Gemini 2"></a>Gemini 2</h3><p>奥比中光Gemini 2是一款双目结构光3D相机，集成了光学，红外，IMU传感器，可以提供RGB和深度图像。</p><blockquote><p>完整规格表：<a target="_blank" rel="noopener" href="https://www.orbbec.com.cn/index/Product/info.html?cate=38&id=51">https://www.orbbec.com.cn/index/Product/info.html?cate=38&amp;id=51</a></p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250505105259308.png" alt="image-20250505105259308"></p><p>Gemini 2为开发者提供了<a target="_blank" rel="noopener" href="https://github.com/orbbec/pyorbbecsdk/tree/main">SDK</a>，可以为开发者处理一些烦杂的底层调用，专注于软件的开发，可以参见其文档：</p><p><a target="_blank" rel="noopener" href="https://github.com/orbbec/pyorbbecsdk/tree/v2-main">https://github.com/orbbec/pyorbbecsdk/tree/v2-main</a></p><blockquote><p>pyorbbecsdk推出了v2版本的sdk，Gemini 2和335更推荐使用v2版本的SDK</p></blockquote><p><a target="_blank" rel="noopener" href="https://orbbec.github.io/pyorbbecsdk/index.html">https://orbbec.github.io/pyorbbecsdk/index.html</a></p><p>如果熟悉c++还可以查到底层的一些API接口，一般在python中都有对应的绑定实现<a target="_blank" rel="noopener" href="https://orbbec.github.io/docs/OrbbecSDKv2/index.html">https://orbbec.github.io/docs/OrbbecSDKv2/index.html</a></p><blockquote><p>当然，其也支持一些Linux下标准的硬件控制，可以利用标准接口和厂商的接口实现一些高级功能</p></blockquote><p>Gemini 2提供的输出有：</p><ul><li><p>RGB图像帧</p></li><li><p>深度图像帧(可以与RGB图像同步)</p></li><li><p>红外图像帧</p></li><li><p>空间坐标和加速度</p><blockquote><p>值得注意的是，由于IMU的频率过高，输出的空间坐标和加速度可能存在与图像帧对不上的问题，如果需要使用的话需要经过后处理</p></blockquote></li></ul><p>其中的SDK还提供了一些非常有用的功能：</p><ul><li>RGB图像帧与深度图像帧同步(硬件&#x2F;软件)</li><li>自动曝光，对焦控制</li><li>镜头保护</li></ul><h1 id="平台搭建"><a href="#平台搭建" class="headerlink" title="平台搭建"></a>平台搭建</h1><p>现代嵌入式设备已经发展得相对复杂，性能也有了极大的提升，面向任务也会更加复杂，除了一些极端环境，已经不需要考虑直接控制设备进行板级开发，一般使用硬件提供的平台或者SDK去完成对应的任务</p><blockquote><p>板级开发一般涉及到内存管理，设备地址读写等操作，正确配置可以在资源极端情况下做到更好的性能</p></blockquote><h3 id="Jetson系统配置"><a href="#Jetson系统配置" class="headerlink" title="Jetson系统配置"></a>Jetson系统配置</h3><h5 id="版本选择"><a href="#版本选择" class="headerlink" title="版本选择"></a>版本选择</h5><p>NVIDIA官方对Jetson设备提供的软件支持是L4T BSP(板级支持包，包括底层驱动，GPU控制等)和JetPack(软件环境，包括CUDA，CUDNN等上层工具)，其包含了运行在一个定制Ubuntu系统上的一系列软件。</p><p>BSP和JetPack都可以通过<a target="_blank" rel="noopener" href="https://developer.nvidia.com/sdk-manager">Nvidia SDK Manager</a>完成安装，需要注意的是，SDK Manager只能在Linux环境下运行，并且，对于系统版本有明确的对应要求：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250505110819617.png" alt="image-20250505110819617"></p><blockquote><p>注意，软件和内容的下载可能需要科学上网</p></blockquote><p>另外，JetPack的版本对于所运行的设备次代也有要求：</p><ul><li>JetPack 4.x支持的设备为Jetson Xavier NX 系列、Jetson TX2 系列、Jetson AGX Xavier 系列、Jetson Nano、Jetson TX1</li><li>JetPack 5.x支持的设备为Jetson AGX Orin 系列、Jetson Orin NX 系列、Jetson Orin Nano 系列、Jetson Xavier NX 系列、Jetson AGX Xavier 系列</li><li>JetPack 6.x支持的设备为Jetson AGX Orin 系列、Jetson Orin NX 系列、Jetson Orin Nano 系列</li></ul><p>整体上就是4支持原始系列，5支持Xavier系列，6支持Orin系列，可以按照这个选择</p><blockquote><p>尽管可能有其它支持，例如5支持Orin系列，但实际支持并不完整，有许多BUG，对于上层软件也有一定限制，因此尽可能按照上面选择</p><p>如果你清楚自己在做什么，那么可以使用命令行的方式进行安装，可以参照<a target="_blank" rel="noopener" href="https://docs.nvidia.com/jetson/archives/r36.4.3/DeveloperGuide/IN/QuickStart.html#to-flash-the-jetson-developer-kit-operating-software">https://docs.nvidia.com/jetson/archives/r36.4.3/DeveloperGuide/IN/QuickStart.html#to-flash-the-jetson-developer-kit-operating-software</a></p></blockquote><p>因此，对于所使用的Jetson Orin NX&#x2F;AGX Orin，所选择的上位机系统为Ubuntu 22.04，JetPack版本为6.2</p><h5 id="烧写系统和组件"><a href="#烧写系统和组件" class="headerlink" title="烧写系统和组件"></a>烧写系统和组件</h5><p>首先需要将Jetson，将开发板与上位机连接，并连接网络，设置为恢复模式，然后打开SDK Manager</p><blockquote><p>对于Jetson NX来说，需要利用杜邦线连接Rec Mode和GND阵脚，然后通电</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513155207939.png" alt="image-20250513155207939" style="zoom:33%"><p>对于Jetson AGX来说，按住Rec按钮，然后点击电源按钮开机</p></blockquote><p>通常来说，只需要选取Jetson系列，Jetson模组(一般无需在上位机上安装软件)进行安装，如下图所示：</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513143624599.png" alt="image-20250513143624599" style="zoom:33%"><p>然后，在第二步选择具体组件开始烧写(可以选择全部，避免之后重新安装)</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513150827091.png" alt="image-20250513150827091" style="zoom:33%"><p>中途会让你设置用户名和密码，填入即可：</p><blockquote><p>对于其中的IP地址，可以保留默认的192.168.55.1，即直接通过usb连接，也可以在路由器中查到开发板的ip地址</p><p>一般OEM配置选择Pre-Config，runtime会在烧写完成后第一次开机进行配置</p><p>对于存储介质选择，AGX需要选择板载的eMMC，然后在烧写完成配置nvme或者利用<a target="_blank" rel="noopener" href="https://github.com/jetsonhacks/rootOnNVMe">工具</a>迁移到NVMe硬盘上，否则可能导致烧写失败</p></blockquote><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513143743334.png" alt="image-20250513143743334" style="zoom:33%"><blockquote><p>很多时候，由于网络问题和上位机与Jetson的连接问题，会导致烧写失败，可以重复上述步骤，如果持续错误，需要确认是否有步骤错误或者在官方论坛询问</p></blockquote><p>等待烧写完成之后开机即可</p><h5 id="初始化配置"><a href="#初始化配置" class="headerlink" title="初始化配置"></a>初始化配置</h5><p>和许多Linux系统一样，Jetson同样支持界面模式(graphics mode)和命令行模式(head-less mode)，在开机时，Jetson会检测视频输出接口(DP或HDMI)是否接有设备，如果没有，就自动进入命令行模式</p><blockquote><p>命令行模式下，无法启用界面模式，VNC等界面相关组件也不可用</p></blockquote><p>在插入视频接口的情况下，使用界面模式，如果需要使用到界面模式的功能，在完成初始设定后，需要额外：</p><ul><li><p>确认网络连接</p></li><li><p>关闭屏幕保护和休眠，在右上角设置的隐私中，可以看到屏幕选项，关闭相关设置，否则可能会引起一些故障</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/1747122327068.jpg" alt="1747122327068" style="zoom:33%"></li></ul><p>在没有插入视频接口的情况下，直接进入命令行模式，没有图形界面输出，可以使用ssh登陆系统，用户名和密码为之前的设置</p><p>如果你熟悉linux，对于系统更新，可以进行更新，最好不要执行apt autoremove命令来清除多余的包，可能包含一些SDK Manager的包，如果不熟悉linux，可以不进行系统更新</p><blockquote><p>一些基础的Linux教程可以参考<a target="_blank" rel="noopener" href="https://www.runoob.com/linux/linux-tutorial.html">https://www.runoob.com/linux/linux-tutorial.html</a></p></blockquote><h5 id="VSCode连接"><a href="#VSCode连接" class="headerlink" title="VSCode连接"></a>VSCode连接</h5><p>对于开发板的开发来说，可以在上位机上使用VSCode进行开发，这样只需要网络通畅，就能随时随地进行开发，可以在<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq812457115/article/details/135533373">https://blog.csdn.net/qq812457115/article/details/135533373</a>找到如何进行</p><h5 id="安装jtop"><a href="#安装jtop" class="headerlink" title="安装jtop"></a>安装jtop</h5><p>jtop是一个专属于Jetson系列的性能监控，配置工具，极其推荐安装，使用以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install -U jetson-stats</span><br></pre></td></tr></table></figure><p>值得注意的是，由于需要获取系统配置，需要重启之后才能使用</p><p>其界面如下，实时显示了系统的信息和负载情况，可以使用键盘和鼠标进行控制</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513162828865.png" alt="image-20250513162828865"></p><p>在6.CTRL中，我们对系统状态进行控制，打开Jetson Clocks并设置为自启动，可以自动控制Jetson的频率，点击Jetson Clocks处的inactivate和disable改为activate和enable：</p><p>打开前：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513162905686.png" alt="image-20250513162905686"></p><p>打开后：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513163134566.png" alt="image-20250513163134566"></p><h5 id="VNC配置"><a href="#VNC配置" class="headerlink" title="VNC配置"></a>VNC配置</h5><p>VNC是一种Linux下远程桌面的互联协议，可以在远程查看Linux的桌面环境，这尤其在进行一些结果可视化，界面调试等工作的时候非常好用</p><p>但值得注意的是，由于Jetson开发板的设计缺陷或者默认Xorg服务的配置错误，Jetson必须在进入界面模式，连接显示器输出，并且显示器打开的情况下才能使用VNC访问桌面环境(还记得之前的关闭休眠吗？)，其它情况下，就会黑屏或者只显示一个英伟达的图标，无法连接桌面</p><p>首先，需要在Jetson的命令行中安装VNC服务并默认启动VNC的服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Gnome桌面</span></span><br><span class="line">sudo apt install vino</span><br><span class="line">cd /usr/lib/systemd/user/graphical-session.target.wants</span><br><span class="line">sudo ln -s ../vino-server.service ./.</span><br></pre></td></tr></table></figure><blockquote><p>值得注意的是，默认服务可能有时无法启动，尤其是在长时间没有输密码进入桌面或者有网络问题时，很容易启动失败，需要手动确认5900端口是否打开</p></blockquote><p>然后，需要配置VNC服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gsettings set org.gnome.Vino prompt-enabled false	# 有新连接时不需要得到弹窗提示的允许，方便远程连接</span><br><span class="line">gsettings set org.gnome.Vino require-encryption false	# 不需要加密，即提升性能和兼容性</span><br></pre></td></tr></table></figure><p>再设定连接VNC需要的密码，需要把其中的thepassword替换成你自己的密码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Replace thepassword with your desired password</span></span><br><span class="line">gsettings set org.gnome.Vino authentication-methods &quot;[&#x27;vnc&#x27;]&quot;</span><br><span class="line">gsettings set org.gnome.Vino vnc-password $(echo -n &#x27;thepassword&#x27;|base64)</span><br></pre></td></tr></table></figure><p>最后，重启之后可以使用客户端连上开发板</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250513202818445.png" alt="image-20250513202818445"></p><blockquote><p>值得注意的是，某些VNC客户端在连接Jetson时会出现异常卡顿的现象，这可能与JPEG压缩等有关，使用一些其它的客户端可能缓解问题，因此只建议使用VNC做调试</p></blockquote><h3 id="工具包配置"><a href="#工具包配置" class="headerlink" title="工具包配置"></a>工具包配置</h3><p>在Jetson上进行开发需要安装一些机器学习相关的软件和库，包括Miniconda，pytorch，ultralytics等等</p><h5 id="安装Miniconda"><a href="#安装Miniconda" class="headerlink" title="安装Miniconda"></a>安装Miniconda</h5><p>Miniconda是一个好用的Python环境管理工具，可以把不同版本的Python工具进行隔离，方便进行管理</p><blockquote><p>相对于Anaconda，其只去除了一些图形化的选项，只保留了核心工具，比较适合Jetson这种资源受限的平台</p></blockquote><p>可以在<a target="_blank" rel="noopener" href="https://www.anaconda.com/docs/getting-started/miniconda/install#aws-graviton2-arm-64">https://www.anaconda.com/docs/getting-started/miniconda/install#aws-graviton2-arm-64</a>找到详细的安装步骤，注意，由于Jetson使用arm架构的CPU，所以需要选择ARM64架构的安装包</p><p>安装成功后，重新进入终端，就可以看到conda的环境提示符：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250514175915839.png" alt="image-20250514175915839"></p><p>conda的具体使用可以参考<a target="_blank" rel="noopener" href="https://www.runoob.com/python-qt/anaconda-tutorial.html">https://www.runoob.com/python-qt/anaconda-tutorial.html</a>中conda命令一章</p><p>由于conda的源在国内访问不稳定，可以将conda和pip的源更换为国内源，具体步骤可以参考清华源的指引：</p><p><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/</a></p><p><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p><p>对于环境的创建，推荐使用Python3.10进行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n general_torch27_py310_cu126 python=3.10</span><br></pre></td></tr></table></figure><p>激活环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate general_torch27_py310_cu126</span><br></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250514200052608.png" alt="image-20250514200052608" style="zoom:50%"><h5 id="安装pytorch"><a href="#安装pytorch" class="headerlink" title="安装pytorch"></a>安装pytorch</h5><p>由于Jetson的硬件结构(arm64)和访存结构(CPU和GPU共用显存)与传统的工作站和服务器不同，Jetson需要特定的CUDA和Pytorch，CUDA在前文Jetson系统烧写的时候已经安装，与系统版本绑定，例如Jetpack 6.2版本使用的CUDA就是12.6版本</p><blockquote><p>推荐使用CUDA 12.6版本对应的Pytorch，否则可能会在需要实时编译的库中遇到问题</p></blockquote><p>而对于Pytorch，也需要安装针对于Jetson的特定版本，具体安装文档可以查询：</p><p>6.0之后：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform/index.html">https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform/index.html</a></p><p>6.0以前：<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048">https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048</a></p><p>根据文档中的规格表和Jetpack版本表可以知道，对于6.2版本，可以安装<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/commit/79aa17489c3fc5ed6d5e972e9ffddf73e6dd0a5c">2.7.0a0+79aa17489c</a>版本的pytorch，具体如下：</p><ol><li><p>安装libopenblas-dev(已经在烧写时安装)</p></li><li><p>安装cusparselt</p><p>可以参照<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cusparselt-downloads?target_os=Linux&target_arch=aarch64-jetson&Compilation=Native&Distribution=Ubuntu&target_version=22.04&target_type=deb_local">https://developer.nvidia.com/cusparselt-downloads?target_os&#x3D;Linux&amp;target_arch&#x3D;aarch64-jetson&amp;Compilation&#x3D;Native&amp;Distribution&#x3D;Ubuntu&amp;target_version&#x3D;22.04&amp;target_type&#x3D;deb_local</a>给出的命令进行安装</p></li><li><p>安装pytorch包</p><blockquote><p>值得注意的是，针对于Jetpack 6.2的pytorch包已经发出，可参见<a target="_blank" rel="noopener" href="https://pypi.jetson-ai-lab.dev/jp6/cu126">https://pypi.jetson-ai-lab.dev/jp6/cu126</a>，但官方手册还未同步</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载包</span></span><br><span class="line">wget https://pypi.jetson-ai-lab.dev/jp6/cu126/+f/6ef/f643c0a7acda9/torch-2.7.0-cp310-cp310-linux_aarch64.whl#sha256=6eff643c0a7acda92734cc798338f733ff35c7df1a4434576f5ff7c66fc97319</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装numpy，注意，numpy更新了2的大版本，会导致很多库现在不支持</span></span><br><span class="line">pip install &#x27;numpy&lt;2&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装pytorch</span></span><br><span class="line">export TORCH_INSTALL=./torch-2.7.0-cp310-cp310-linux_aarch64.whl</span><br><span class="line">pip install --no-cache $TORCH_INSTALL</span><br></pre></td></tr></table></figure></li><li><p>可以使用如下代码验证安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"><span class="built_in">print</span>(torch.backends.cudnn.is_available())</span><br></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250514200343567.png" alt="image-20250514200343567" style="zoom:50%"></li><li><p><del>安装torchvision</del></p><p><del>torchvision是专注于图像处理的torch相关库，提供了视觉处理相关的数据处理，预训练模型等</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torchvision --index-url https://pypi.jetson-ai-lab.dev/jp6/cu126/</span><br></pre></td></tr></table></figure></li><li><p>编译并安装torchvision</p><p>由于nvidia官方在编译torchvision的包时，并没有加入对于<code>torchvision::nms</code>的CUDA支持，所以在使用yolo进行目标检测时是会报错的，有两种选择：</p><ul><li>针对目标检测重新创建环境，安装属于Jetpack6.1的torch和torchvision，实测可以通过<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/yolo-incompatible-with-jetpack-6-2-jetson-orin-nano-super/321078">https://forums.developer.nvidia.com/t/yolo-incompatible-with-jetpack-6-2-jetson-orin-nano-super/321078</a></li><li>重新编译torchvision(需要修改编译选项)，可以参考<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048">https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048</a></li></ul></li></ol><h5 id="安装ultralytics"><a href="#安装ultralytics" class="headerlink" title="安装ultralytics"></a>安装ultralytics</h5><p>ultralytics集成了YOLO系列的众多模型，包括常用的YOLOv8系列，YOLOv11系列，并且对于端侧部署做了优化，是最常用的单阶段目标检测模型库，其也在最近支持了Orin系列，可以参考其<a target="_blank" rel="noopener" href="https://docs.ultralytics.com/">官方文档1</a>，<a target="_blank" rel="noopener" href="https://docs.ultralytics.com/guides/nvidia-jetson/">官方文档2</a></p><p>首先需要安装一些依赖，自动搜索这些依赖会达到搜索最大深度，无法完成搜索和安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载这些包，可以从https://pypi.jetson-ai-lab.dev/jp6/cu126找到</span></span><br><span class="line">wget https://pypi.jetson-ai-lab.dev/jp6/cu126/+f/5e2/8f3dca560ab6f/jaxlib-0.6.0.dev20250414-cp310-cp310-manylinux2014_aarch64.whl#sha256=5e28f3dca560ab6fe4fa377029b5cf6f43114bf8f821093c75f2099106337e7d</span><br><span class="line">wget https://pypi.jetson-ai-lab.dev/jp6/cu126/+f/53f/9bd27343d2c42/jax_cuda12_pjrt-0.6.0.dev20250414-py3-none-manylinux2014_aarch64.whl#sha256=53f9bd27343d2c42ef90c5e17205caf1038f02197e67be517da7c54798f491e0</span><br><span class="line">wget https://pypi.jetson-ai-lab.dev/jp6/cu126/+f/b19/f96a75d0256e7/jax_cuda12_plugin-0.6.0.dev20250414-cp310-cp310-manylinux2014_aarch64.whl#sha256=b19f96a75d0256e7c08612ee3bb90bdedf3b6d60f091822dcb1c610b2da2f258</span><br><span class="line">wget https://pypi.jetson-ai-lab.dev/jp6/cu126/+f/72c/84d2296f2207a/jax-0.6.0.dev20250414+6ca623f-py3-none-any.whl#sha256=72c84d2296f2207ac08b8976150b1437bcd75dd562b2f69608741f3732f96259</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装</span></span><br><span class="line">pip install jax_cuda12_pjrt-0.6.0.dev20250414-py3-none-manylinux2014_aarch64.whl</span><br><span class="line">pip install jax_cuda12_plugin-0.6.0.dev20250414-cp310-cp310-manylinux2014_aarch64.whl</span><br><span class="line">pip install jaxlib-0.6.0.dev20250414-cp310-cp310-manylinux2014_aarch64.whl</span><br><span class="line">pip install jax-0.6.0.dev20250414+6ca623f-py3-none-any.whl</span><br></pre></td></tr></table></figure><p>然后就可以直接使用命令安装，此处的export包含了对于onnx，torchscript等移动端模型的优化支持</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ultralytics[export]</span><br></pre></td></tr></table></figure><blockquote><p>一些库需要现场编译，所以可能会很慢</p></blockquote><blockquote><p>值得注意的是，由于一些库的依赖过深，会导致达到搜索上限，可以先安装基础的ultralytics，然后运行如下命令让ultralytics自动安装onnx组件，其它组件也是类似</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install ultralycis</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自动安装onnx组件</span></span><br><span class="line">yolo export model=yolo11n.pt format=onnx</span><br></pre></td></tr></table></figure><p>但对于tensorrt组件，需要GPU支持，同时需要onnxruntime-gpu，而上方只会安装onnxruntime，需要手动安装gpu版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://pypi.jetson-ai-lab.dev/jp6/cu126/+f/869/e41abdc35e093/onnxruntime_gpu-1.22.0-cp310-cp310-linux_aarch64.whl#sha256=869e41abdc35e09345876f047fce49267d699df3e44b67c2518b0469739484ff</span><br><span class="line">pip install onnxruntime_gpu-1.22.0-cp310-cp310-linux_aarch64.whl</span><br></pre></td></tr></table></figure><p>在安装tensorrt时，由于官方源中没有针对Jetson的tensorrt，需要在<a target="_blank" rel="noopener" href="https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.l4t.aarch64-gnu.cuda-12.6.tar.gz">https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.l4t.aarch64-gnu.cuda-12.6.tar.gz</a>下载tar包之后，解压后找到其中的python文件夹中的包进行安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install ./tensorrt_lean-10.3.0-cp310-none-linux_aarch64.whl</span><br><span class="line">pip install ./tensorrt_dispatch-10.3.0-cp310-none-linux_aarch64.whl</span><br><span class="line">pip install ./tensorrt-10.3.0-cp310-none-linux_aarch64.whl</span><br></pre></td></tr></table></figure></blockquote><h5 id="安装mmcv和mmdet-废弃，暂时无法使用"><a href="#安装mmcv和mmdet-废弃，暂时无法使用" class="headerlink" title="安装mmcv和mmdet(废弃，暂时无法使用)"></a>安装mmcv和mmdet(废弃，暂时无法使用)</h5><p>mmcv系列是商汤开源的一系列深度学习工具，其中，mmdet和mmrotate是专注于目标检测的部分，其中包含了许多二阶段目标检测网络，其使用可以参考<a target="_blank" rel="noopener" href="https://mmcv.readthedocs.io/en/latest/">mmcv官方文档</a>和<a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/en/latest/">mmdet官方文档</a></p><p>对于其安装，可以按照以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pip install -U openmim</span><br><span class="line">mim install mmengine</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注意，对于mmcv，由于没有针对于Jetson的mmcv二进制包，所以需要自动编译，会等较长时间</span></span><br><span class="line">conda install -c conda-forge gxx_linux-64	# 修复编译时存在的错误</span><br><span class="line">cd $&#123;miniconda_install_dir&#125;/envs/$&#123;env_name&#125;/bin	# 需要把其中的miniconda_install_dir替换为conda安装的路径，env_name替换为环境名</span><br><span class="line">ln -s $&#123;miniconda_install_dir&#125;/envs/$&#123;env_name&#125;/bin/x86_64-conda_cos6-linux-gnu-g++ g++</span><br><span class="line">mim install &quot;mmcv==2.1.0&quot;	</span><br><span class="line"></span><br><span class="line">mim install mmdet</span><br></pre></td></tr></table></figure><h5 id="安装带有CUDA加速的opencv"><a href="#安装带有CUDA加速的opencv" class="headerlink" title="安装带有CUDA加速的opencv"></a>安装带有CUDA加速的opencv</h5><p>在Jetpack中，默认包含了opencv的库，但是，其默认不支持CUDA加速</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250515200423815.png" alt="image-20250515200423815" style="zoom:50%"><p>在图像处理中，Jetson较弱的CPU会带来较长的处理时间，好在opencv可以支持CUDA加速，可以使用<a target="_blank" rel="noopener" href="https://github.com/Qengineering/Install-OpenCV-Jetson-Nano">第三方的脚本</a>进行安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/Qengineering/Install-OpenCV-Jetson-Nano/refs/heads/main/OpenCV-4-10-0.sh</span><br><span class="line">bash OpenCV-4-10-0.sh</span><br></pre></td></tr></table></figure><blockquote><p>注意可能遇到网络问题</p></blockquote><p>在安装之后，可以看到jtop界面也发生了变化：</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250516104209084.png" alt="image-20250516104209084" style="zoom:50%"><p>opencv中支持cuda的操作可以在<code>cv2.cuda</code>中进行调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="built_in">print</span>(cv2.getBuildInformation())</span><br></pre></td></tr></table></figure><blockquote><p>注意先运行<code>echo &#39;export OPENBLAS_CORETYPE=ARMV8&#39; &gt;&gt; ~/.bashrc</code>并重启终端来指示架构</p></blockquote><h5 id="安装相机pyorbbec-SDK"><a href="#安装相机pyorbbec-SDK" class="headerlink" title="安装相机pyorbbec SDK"></a>安装相机pyorbbec SDK</h5><p>对于Gemini 2相机，可以安装pyorbbec sdk来进行控制</p><p>安装前需要安装一些依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install build-essential	# cmake</span><br><span class="line">pip install pybind11	# pybind11</span><br></pre></td></tr></table></figure><p>安装首先需要克隆官方代码并切换到v2版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/orbbec/pyorbbecsdk.git</span><br><span class="line">cd pyorbbecsdk</span><br><span class="line">git checkout v2-main</span><br></pre></td></tr></table></figure><p>然后由于是使用了conda，需要在CmakeList.txt中修改python路径，可以使用nano进行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nano</span><br><span class="line">nano CMakeLists.txt</span><br></pre></td></tr></table></figure><p>在35行的<code>find_package(Python3 REQUIRED COMPONENTS Interpreter Development)</code>之前加入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set(Python3_ROOT_DIR &quot;&quot;) # 需要在引号中填入环境的路径</span><br><span class="line">set(pybind11_DIR &quot;$&#123;Python3_ROOT_DIR&#125;/lib/python3.10/site-packages/pybind11/share/cmake/pybind11&quot;)</span><br></pre></td></tr></table></figure><p>接下来需要安装依赖环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><blockquote><p>注意，gcc的版本可能会对其中的udev的编译造成影响</p></blockquote><p>进行编译：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake -Dpybind11_DIR=`pybind11-config --cmakedir` ..</span><br><span class="line">make -j8</span><br><span class="line">make install</span><br><span class="line">echo &#x27;export PYTHONPATH=$PYTHONPATH:$(pwd)/install/lib/&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><blockquote><p>这一步之后需要重启终端加载环境变量</p></blockquote><p>安装为python包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">pip3 install wheel</span><br><span class="line">python3 setup.py bdist_wheel</span><br><span class="line">cd dist</span><br><span class="line">pip3 install pyorbbecsdk-2.0.10-cp310-cp310-linux_aarch64.whl</span><br></pre></td></tr></table></figure><p>值得注意的是，由于需要与硬件进行交互，需要设置udev规则来进行允许：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo bash ./scripts/install_udev_rules.sh</span><br><span class="line">sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</span><br></pre></td></tr></table></figure><p>然后摄像机就可以正常工作了，例如，可以调用官方的示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 examples/depth_viewer.py</span><br></pre></td></tr></table></figure><blockquote><p>需要注意的是，这需要显示画面，即Jetson需要有视频输出</p></blockquote><h1 id="平台使用入门"><a href="#平台使用入门" class="headerlink" title="平台使用入门"></a>平台使用入门</h1><h3 id="相机调用-pyorbbec-SDK使用"><a href="#相机调用-pyorbbec-SDK使用" class="headerlink" title="相机调用(pyorbbec SDK使用)"></a>相机调用(pyorbbec SDK使用)</h3><p>对于Gemini 2相机的调用，有两种形式：</p><ul><li>利用通用接口，即Linux的设备管理和图像处理库(一般是opencv)，具体可以参考<a target="_blank" rel="noopener" href="https://my.oschina.net/emacs_8787797/blog/17264507">教程</a></li><li>利用相机提供的SDK进行控制，例如，Gemini 2提供了c++，python等语言的SDK支持，pyorbbec SDK就是针对于Python的SDK，提供了相机初始化，曝光控制，传感器控制等选项</li></ul><blockquote><p>有关于Python的教程可以查看<a target="_blank" rel="noopener" href="https://www.runoob.com/python3/python3-tutorial.html">https://www.runoob.com/python3/python3-tutorial.html</a></p></blockquote><h5 id="pyorbbec-SDK概念"><a href="#pyorbbec-SDK概念" class="headerlink" title="pyorbbec SDK概念"></a>pyorbbec SDK概念</h5><p>在pyorbbec SDK中，为了更方便操作设备，会分为几个层级，按照流程进行操作，从最接近底层到应用层分别为：</p><blockquote><p>官方文档<a target="_blank" rel="noopener" href="https://orbbec.github.io/OrbbecSDK/doc/tutorial/English/OverviewDocument.html">https://orbbec.github.io/OrbbecSDK/doc/tutorial/English/OverviewDocument.html</a></p></blockquote><ul><li><p>Context：整个处理上下文，提供了一组设置，包括设备访问，状态变化回调，日志等</p><blockquote><p>一般来说，由于Context过于底层，不建议使用Context操控设备，在创建pipeline时，会自动隐含创建一个Context</p></blockquote></li><li><p>Device：设备对象，一个实际的硬件设备对应一个Device对象，例如一个Gemini 2，该对象用于获取设备的相关信息，并控制设备的属性(传感器等)</p><blockquote><p>同样，也不建议使用Device操作设备进行应用编写</p></blockquote></li><li><p>Sensor：传感器对象，可以理解为实际设备的子设备，例如RGB传感器，深度传感器等，一个Device可能包含多个Device</p><blockquote><p>同样，也不建议使用Sensor操作设备进行应用编写</p></blockquote></li><li><p><strong>Pipeline</strong>：流程对象，对底层结构进行了封装，包含了可以快速访问的结构，功能简洁，方便进行开发</p></li><li><p>Config：整体控制对象，包含了对于整个设备和流程的控制，例如光学传感器的分辨率，刷新率</p></li><li><p>Profile：设备和流程控制对象，具体控制流程中的参数，例如，VideoStreamProfile控制拍摄视频的分辨率，刷新率等等</p></li><li><p>Frame：视频中的一帧数据，同时包含该帧数据的相关信息，比如时间戳，图像元数据，类型等</p></li></ul><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/OrbbecSDK-Get-Frame-Sequence-Diagram.png" alt="OrbbecSDK Get Frame Sequence Diagram"></p><h5 id="获取设备信息"><a href="#获取设备信息" class="headerlink" title="获取设备信息"></a>获取设备信息</h5><p>即示例1.get_device_info.py</p><p>设备级别的控制和访问需要Context进行，需要通过创建Context访问Device，进而访问设备信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">ctx = Context()</span><br><span class="line">...</span><br><span class="line">device = ctx.query_devices()[<span class="number">0</span>]</span><br><span class="line">...</span><br><span class="line">device_info = device.get_device_info()</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h5 id="访问内参"><a href="#访问内参" class="headerlink" title="访问内参"></a>访问内参</h5><p>相机的内参即焦距(fx, fy)、主点坐标(cx, cy)、畸变参数，可以把坐标从相机坐标系转换到像素坐标系中，与相机本身绑定，可以通过两种方式获取内参，即Pipeline和Profile</p><blockquote><p>但值得注意的是，相机内参需要确定分辨率等信息，Pipeline获取的是图像参数与相机内参的对应列表，Profile获取的是这种配置情况下的内参</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过Pipeline获取了列表</span></span><br><span class="line">camera_param = pipeline.get_camera_param()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定好Profile后获取值</span></span><br><span class="line">profile_list = pipeline.get_stream_profile_list(OBSensorType.COLOR_SENSOR)</span><br><span class="line">color_profile = profile_list.get_default_video_stream_profile()</span><br><span class="line">color_intrinsics = color_profile.get_intrinsic()</span><br></pre></td></tr></table></figure><h5 id="时钟同步"><a href="#时钟同步" class="headerlink" title="时钟同步"></a>时钟同步</h5><p>如果需要自主控制成像(固定时间，RGB-D对齐。。)，需要在代码中进行精确时钟同步：</p><blockquote><p>如果硬件自动同步就不需要</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyorbbecsdk <span class="keyword">import</span> *</span><br><span class="line">pipeline = Pipeline()</span><br><span class="line">device = pipeline.get_device()</span><br><span class="line">device.timer_sync_with_host()</span><br></pre></td></tr></table></figure><h5 id="开启相机"><a href="#开启相机" class="headerlink" title="开启相机"></a>开启相机</h5><p>我们使用一个简单的示例来展示如何进行相机控制，控制有两种方式，一种是主动式，一种是使用回调</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250516181520502.png" alt="image-20250516181520502" style="zoom:33%"><p>主动控制：</p><p>首先需要引入pyorbbec包，在其中，对底层的c++进行了封装，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyorbbecsdk <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><p>然后，需要创建pipeline，来控制整个流程，并对参数进行配置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline()</span><br><span class="line"><span class="variable language_">self</span>.config = Config()</span><br></pre></td></tr></table></figure><p>接下来配置相机信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">profile_list = pipeline.get_stream_profile_list(OBSensorType.COLOR_SENSOR)</span><br><span class="line">color_profile = profile_list.get_video_stream_profile(resolution, <span class="number">0</span>, OBFormat.RGB, fps)</span><br></pre></td></tr></table></figure><p>开始工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config.enable_stream(color_profile)</span><br><span class="line">pipeline.start(<span class="variable language_">self</span>.config)</span><br></pre></td></tr></table></figure><p>对每一帧进行处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  frames: FrameSet = pipeline.wait_for_frames(<span class="number">100</span>)</span><br><span class="line">  color_frame = frames.get_color_frame()</span><br><span class="line">  <span class="comment"># covert to RGB format</span></span><br><span class="line">  color_image = frame_to_bgr_image(color_frame)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><h5 id="中断和回调函数"><a href="#中断和回调函数" class="headerlink" title="中断和回调函数"></a>中断和回调函数</h5><p>对于相机这种外围设备，其速度与CPU有差异(RGB相机获取图像很慢，imu等频率很高)，由此，如果使用循环一直等待然后进行处理，CPU必定需要进行等待，导致当前进程一直停滞，一种解决方法就是利用<strong>中断机制</strong>，使用回调函数进行处理</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/1.webp" alt="1"></p><p>如上图中，CPU可以继续执行其它指令，在收到中断(图像完成等)，执行回调函数(预先写好的处理程序)，然后继续执行指令</p><p>中断的核心在于回调函数的书写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_color_depth_frame_callback</span>(<span class="params">data_queue: AlignedQueue</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">color_depth_frame_callback</span>(<span class="params">frame: FrameSet</span>):</span><br><span class="line">        <span class="keyword">if</span> frame <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        depth_frame = frame.get_depth_frame()</span><br><span class="line">        color_frame = frame.get_color_frame()</span><br><span class="line">        <span class="keyword">if</span> depth_frame <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> color_frame <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        depth_image = frame_to_gray_image(depth_frame)</span><br><span class="line">        center_distnce = calc_distance([<span class="number">0.5</span>, <span class="number">0.5</span>], depth_frame)</span><br><span class="line">        color_image = frame_to_bgr_image(color_frame)</span><br><span class="line"></span><br><span class="line">        data_queue.store_one_frame(color_image, depth_image, center_distnce)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> color_depth_frame_callback</span><br></pre></td></tr></table></figure><p>然后只需要在开始执行之后，把回调函数注册到进程中，在中断时就会自动执行回调函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipeline.start(<span class="variable language_">self</span>.config, call_back)</span><br></pre></td></tr></table></figure><h5 id="硬件同步"><a href="#硬件同步" class="headerlink" title="硬件同步"></a>硬件同步</h5><p>可以看到，在多个传感器或设备同时工作时，其获得的时间戳可能是不相同的，由此，在这样的场景中，就需要对传感器进行同步，同步分为硬件同步和软件同步：</p><ul><li>软件同步：软件同步故名思义，就是利用软件进行判断，通过时间戳，网络命令等协调多个传感器之间的采集</li><li>硬件同步：利用相机本身的硬件，例如内置时钟信号连接，协调多个 传感器之间的采集</li></ul><blockquote><p>一般来说，由于软件同步会造成一定的CPU占用，如果有硬件同步支持，优先使用硬件同步</p></blockquote><p>Gemini 2带有硬件同步的支持，需要手动进行开启，在Config中，有一个额外的get_d2c_depth_profile_list进行设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">hw_d2c_profile_list = pipeline.get_d2c_depth_profile_list(color_profile, OBAlignMode.HW_MODE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable the depth and color streams</span></span><br><span class="line">config.enable_stream(hw_d2c_profile_list[<span class="number">0</span>])</span><br><span class="line">config.enable_stream(color_profile)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the alignment mode to hardware alignment</span></span><br><span class="line">config.set_align_mode(OBAlignMode.HW_MODE)</span><br></pre></td></tr></table></figure><h3 id="图像处理入门"><a href="#图像处理入门" class="headerlink" title="图像处理入门"></a>图像处理入门</h3><h5 id="opencv使用"><a href="#opencv使用" class="headerlink" title="opencv使用"></a>opencv使用</h5><p>opencv是一个图像处理库，其中包含了许多图像处理的方法，主要用于图像加载，预处理，后处理等，也包含许多传统机器学习方法，是图像处理的综合性库，具体使用可以参见<a target="_blank" rel="noopener" href="https://codec.wang/docs/opencv">https://codec.wang/docs/opencv</a>，<a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/index.html">https://docs.opencv.org/4.x/index.html</a>等文档</p><p>这里列出了一些典型的图像操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取</span></span><br><span class="line">img = cv2.imread(path: <span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像(持久显示)</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;image&#x27;</span>,img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存图像</span></span><br><span class="line">cv.imwrite(path: <span class="built_in">str</span>, img)</span><br></pre></td></tr></table></figure><p>包括对于视频的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取摄像头0</span></span><br><span class="line">cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">    <span class="comment"># Capture frame-by-frame</span></span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Our operations on the frame come here</span></span><br><span class="line">    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the resulting frame</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;frame&#x27;</span>,gray)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">        <span class="comment"># release device and destory window</span></span><br><span class="line">        cap.release()</span><br><span class="line">        cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><blockquote><p>保存视频相对麻烦，因为要根据所有视频帧进行编码为一个文件，需要使用<code>VideoWriter</code>对象实现</p></blockquote><p>实际上，opencv中更加复杂的是各种图像&#x2F;视频处理：</p><p>例如：</p><ul><li><p>变换类：几何变换，边缘滤波，图像分割。。。</p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250517160108167.png" alt="image-20250517160108167" style="zoom:50%"></li><li><p>特征检测类：角点检测，特征追踪，特征匹配。。。</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250517160240932.png" alt="image-20250517160240932"></p></li><li><p>机器学习类：K近邻，SVM。。。</p></li></ul><p>另一方面，在前文进行安装之后，其中的cuda模块也是重要的一环，我们知道，Jetson的CPU实际上相对较弱，而GPU性能较强，所以可以通过GPU进行加速，一般GPU处理涉及到三个步骤：</p><ul><li><p>在GPU的空间内申请空间，把图像上传到GPU显存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Upload the image to the GPU</span></span><br><span class="line">gpu_image = cv2.cuda_GpuMat()</span><br><span class="line">gpu_image.upload(image)</span><br></pre></td></tr></table></figure><blockquote><p>尽管JetsonCPU和GPU物理上共用显存，但似乎为了符合现代计算机结构，在逻辑上还是分了内存和显存，至少opencv还是需要执行相应步骤</p></blockquote></li><li><p>在GPU上进行图像处理</p></li><li><p>将图像下载到内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download the result back to the CPU</span></span><br><span class="line">result_image = gpu_blurred_image.download()</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>由于实际上多出来上传和下载的步骤，小批量图像的简单的操作，例如单张图像施加高斯噪声等，在GPU上会更慢，最适合在GPU上处理的是大批量，高复杂度，并行化的任务</p><p>参见代码opencv 1和2</p></blockquote><h5 id="yolo系列检测模型使用"><a href="#yolo系列检测模型使用" class="headerlink" title="yolo系列检测模型使用"></a>yolo系列检测模型使用</h5><p>对于Jetson这样的设别来说，一类比较适合的任务就是这样的实时视觉检测，ultralytics提供了YOLOv8和YOLOv11两个系列的模型，并提供了边缘计算的支持</p><blockquote><p>对于很多模型来说，其都是在大型服务器上进行训练，机器的吞吐量，读写等方面与端侧不同，对端侧的推理优化是部署的很重要的方面</p></blockquote><p>其边缘计算的能力主要由在Jetson上主要使用TensorRT进行加速，幸运的是，YOLO系列模型封装了相关的函数，可以利用export将标准的YOLO模型转换为TensorRT模型：</p><blockquote><p>可以查询到其参数<a target="_blank" rel="noopener" href="https://docs.ultralytics.com/integrations/tensorrt/#export-arguments">https://docs.ultralytics.com/integrations/tensorrt/#export-arguments</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a YOLO11n PyTorch model</span></span><br><span class="line">model = YOLO(<span class="string">&quot;yolo11n.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Export the model to TensorRT</span></span><br><span class="line">model.export(<span class="built_in">format</span>=<span class="string">&quot;engine&quot;</span>)  <span class="comment"># creates &#x27;yolo11n.engine&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>tensorrt需要对于其中的计算步骤进行自动优化()，会进行层融合，精度校准，动态内存管理等，由此，输入的图像尺寸会固定</p></blockquote><blockquote><p>已知Jetpack 6.2的torch 2.7.0 和torchvision 0.22.0的编译存在问题，与ultralytics不兼容</p></blockquote><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250517210647438.png" alt="image-20250517210647438" style="zoom:50%"> <img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250517210545424.png" alt="image-20250517210545424" style="zoom:50%"><p>可以看到，使用tensorrt优化之后的模型在推理速度上会有较大的提升</p><blockquote><p>在后处理部分，nms大部分还是使用cpu进行</p></blockquote><p>另一方面的选择其实是是否量化，一般在训练时，会采用Float32精度进行，混合训练也只会用到float16，但在推理时，可以使用INT8等更小的类型进行，这也可以使用tensorrt进行实现，只需要在export中进行设定：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;yolov8n.pt&quot;</span>)</span><br><span class="line">model.export(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;engine&quot;</span>,</span><br><span class="line">    int8=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><p>同时，在export出int8时最好指定一个data的数据集，来确定模型的适用范围</p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250517212742016.png" alt="image-20250517212742016"></p><p>这样会有两方面的影响：</p><ul><li>优点：速度快，模型存储占用小，对于资源受限平台友好</li><li>缺点：可能减少泛化性，甚至降低性能</li></ul><img src="./attachments/image-20250517211605180.png" alt="image-20250517211605180" style="zoom:50%"><p>为了进一步降低功耗，部分网络层可以跑在Jetson专属的深度学习加速器上，但可能带来一定的性能下降，同样，也是在export时进行选择，使用device选项：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;yolo11n.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">model.export(<span class="built_in">format</span>=<span class="string">&quot;engine&quot;</span>, device=<span class="string">&quot;dla:0&quot;</span>, half=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>对于视频处理，可以使用deepstream，参考<a target="_blank" rel="noopener" href="https://docs.ultralytics.com/zh/guides/deepstream-nvidia-jetson/#deepstream-configuration-for-yolo11">https://docs.ultralytics.com/zh/guides/deepstream-nvidia-jetson/#deepstream-configuration-for-yolo11</a></p><h1 id="可用资源汇总"><a href="#可用资源汇总" class="headerlink" title="可用资源汇总"></a>可用资源汇总</h1><p>Jetson官方初始文档：<a target="_blank" rel="noopener" href="https://developer.nvidia.cn/embedded/learn/getting-started-jetson">https://developer.nvidia.cn/embedded/learn/getting-started-jetson</a></p><p>Jetson官方文档集：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/jetson/">https://docs.nvidia.com/jetson/</a></p><p>英伟达论坛Jetson专区：<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/c/robotics-edge-computing/jetson-embedded-systems/70">https://forums.developer.nvidia.com/c/robotics-edge-computing/jetson-embedded-systems/70</a></p><p>专属于Jetpack 6的最新包：<a target="_blank" rel="noopener" href="https://pypi.jetson-ai-lab.dev/jp6/cu126">https://pypi.jetson-ai-lab.dev/jp6/cu126</a></p><p>奥比中光官方Github：<a target="_blank" rel="noopener" href="https://github.com/orbbec">https://github.com/orbbec</a></p><p>奥比中光SDK文档：<a target="_blank" rel="noopener" href="https://orbbec.github.io/pyorbbecsdk/index.html">https://orbbec.github.io/pyorbbecsdk/index.html</a></p><p>奥比中光论坛：<a target="_blank" rel="noopener" href="https://developer.orbbec.com.cn/v/forum">https://developer.orbbec.com.cn/v/forum</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top">Chanoch Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top/2025/05/04/Jetson%E6%95%99%E7%A8%8B/">https://blog.chanoch.top/2025/05/04/Jetson%E6%95%99%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.chanoch.top" target="_blank">Chanoch的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Jetson/">Jetson</a><a class="post-meta__tags" href="/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/07/uv%E5%BA%93/" title="uv库使用"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">uv库使用</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/08/31/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv2/" title="[论文笔记] DINOv2"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">[论文笔记] DINOv2</div></div><div class="info-2"><div class="info-item-1">信息Title: DINOv2: Learning Robust Visual Features without Supervision Author: Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Chanoch Li</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/chanochLi" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:kujou@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Jetson%E5%B9%B3%E5%8F%B0%E5%92%8CGemini-2%E7%9B%B8%E6%9C%BA%E5%B9%B3%E5%8F%B0"><span class="toc-number">1.</span> <span class="toc-text">Jetson平台和Gemini 2相机平台</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Jetson-Orin-NX-Jetson-AGX-Orin"><span class="toc-number">1.0.1.</span> <span class="toc-text">Jetson Orin NX&#x2F;Jetson AGX Orin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gemini-2"><span class="toc-number">1.0.2.</span> <span class="toc-text">Gemini 2</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA"><span class="toc-number">2.</span> <span class="toc-text">平台搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Jetson%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE"><span class="toc-number">2.0.1.</span> <span class="toc-text">Jetson系统配置</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%89%88%E6%9C%AC%E9%80%89%E6%8B%A9"><span class="toc-number">2.0.1.0.1.</span> <span class="toc-text">版本选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%83%A7%E5%86%99%E7%B3%BB%E7%BB%9F%E5%92%8C%E7%BB%84%E4%BB%B6"><span class="toc-number">2.0.1.0.2.</span> <span class="toc-text">烧写系统和组件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE"><span class="toc-number">2.0.1.0.3.</span> <span class="toc-text">初始化配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#VSCode%E8%BF%9E%E6%8E%A5"><span class="toc-number">2.0.1.0.4.</span> <span class="toc-text">VSCode连接</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85jtop"><span class="toc-number">2.0.1.0.5.</span> <span class="toc-text">安装jtop</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#VNC%E9%85%8D%E7%BD%AE"><span class="toc-number">2.0.1.0.6.</span> <span class="toc-text">VNC配置</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E5%8C%85%E9%85%8D%E7%BD%AE"><span class="toc-number">2.0.2.</span> <span class="toc-text">工具包配置</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Miniconda"><span class="toc-number">2.0.2.0.1.</span> <span class="toc-text">安装Miniconda</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85pytorch"><span class="toc-number">2.0.2.0.2.</span> <span class="toc-text">安装pytorch</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85ultralytics"><span class="toc-number">2.0.2.0.3.</span> <span class="toc-text">安装ultralytics</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85mmcv%E5%92%8Cmmdet-%E5%BA%9F%E5%BC%83%EF%BC%8C%E6%9A%82%E6%97%B6%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8"><span class="toc-number">2.0.2.0.4.</span> <span class="toc-text">安装mmcv和mmdet(废弃，暂时无法使用)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%B8%A6%E6%9C%89CUDA%E5%8A%A0%E9%80%9F%E7%9A%84opencv"><span class="toc-number">2.0.2.0.5.</span> <span class="toc-text">安装带有CUDA加速的opencv</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E7%9B%B8%E6%9C%BApyorbbec-SDK"><span class="toc-number">2.0.2.0.6.</span> <span class="toc-text">安装相机pyorbbec SDK</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8"><span class="toc-number">3.</span> <span class="toc-text">平台使用入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E6%9C%BA%E8%B0%83%E7%94%A8-pyorbbec-SDK%E4%BD%BF%E7%94%A8"><span class="toc-number">3.0.1.</span> <span class="toc-text">相机调用(pyorbbec SDK使用)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#pyorbbec-SDK%E6%A6%82%E5%BF%B5"><span class="toc-number">3.0.1.0.1.</span> <span class="toc-text">pyorbbec SDK概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF"><span class="toc-number">3.0.1.0.2.</span> <span class="toc-text">获取设备信息</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E5%86%85%E5%8F%82"><span class="toc-number">3.0.1.0.3.</span> <span class="toc-text">访问内参</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%B6%E9%92%9F%E5%90%8C%E6%AD%A5"><span class="toc-number">3.0.1.0.4.</span> <span class="toc-text">时钟同步</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%80%E5%90%AF%E7%9B%B8%E6%9C%BA"><span class="toc-number">3.0.1.0.5.</span> <span class="toc-text">开启相机</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%AD%E6%96%AD%E5%92%8C%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0"><span class="toc-number">3.0.1.0.6.</span> <span class="toc-text">中断和回调函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E5%90%8C%E6%AD%A5"><span class="toc-number">3.0.1.0.7.</span> <span class="toc-text">硬件同步</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8"><span class="toc-number">3.0.2.</span> <span class="toc-text">图像处理入门</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#opencv%E4%BD%BF%E7%94%A8"><span class="toc-number">3.0.2.0.1.</span> <span class="toc-text">opencv使用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#yolo%E7%B3%BB%E5%88%97%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-number">3.0.2.0.2.</span> <span class="toc-text">yolo系列检测模型使用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%AF%E7%94%A8%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB"><span class="toc-number">4.</span> <span class="toc-text">可用资源汇总</span></a></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/" title="[论文笔记] DINOv3">[论文笔记] DINOv3</a><time datetime="2025-08-22T11:20:48.000Z" title="发表于 2025-08-22 19:20:48">2025-08-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dove/" title="[论文笔记] dove">[论文笔记] dove</a><time datetime="2025-07-29T12:59:29.000Z" title="发表于 2025-07-29 20:59:29">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BitNet/" title="[论文笔记] BitNet">[论文笔记] BitNet</a><time datetime="2025-05-26T12:25:14.000Z" title="发表于 2025-05-26 20:25:14">2025-05-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BitNetV2/" title="[论文笔记] BitNetV2">[论文笔记] BitNetV2</a><time datetime="2025-05-26T12:25:14.000Z" title="发表于 2025-05-26 20:25:14">2025-05-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/07/uv%E5%BA%93/" title="uv库使用">uv库使用</a><time datetime="2025-05-07T04:22:39.000Z" title="发表于 2025-05-07 12:22:39">2025-05-07</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"><div id="footer-wrap"><div class="copyright">&copy;2025 By Chanoch Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'all',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
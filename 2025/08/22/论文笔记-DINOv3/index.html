<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>[论文笔记] DINOv3 | Chanoch的博客</title><meta name="author" content="Chanoch Li"><meta name="copyright" content="Chanoch Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="信息Title: DINOv3 Author: Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Mass"><meta property="og:type" content="article"><meta property="og:title" content="[论文笔记] DINOv3"><meta property="og:url" content="https://blog.chanoch.top/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/index.html"><meta property="og:site_name" content="Chanoch的博客"><meta property="og:description" content="信息Title: DINOv3 Author: Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Mass"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog.chanoch.top/img/avatar.png"><meta property="article:published_time" content="2025-08-22T11:20:48.000Z"><meta property="article:modified_time" content="2025-08-31T12:26:40.995Z"><meta property="article:author" content="Chanoch Li"><meta property="article:tag" content="论文阅读"><meta property="article:tag" content="自监督学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.chanoch.top/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[论文笔记] DINOv3",
  "url": "https://blog.chanoch.top/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/",
  "image": "https://blog.chanoch.top/img/avatar.png",
  "datePublished": "2025-08-22T11:20:48.000Z",
  "dateModified": "2025-08-31T12:26:40.995Z",
  "author": [
    {
      "@type": "Person",
      "name": "Chanoch Li",
      "url": "https://blog.chanoch.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="https://blog.chanoch.top/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"[论文笔记] DINOv3",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Chanoch的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">[论文笔记] DINOv3</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">[论文笔记] DINOv3</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-22T11:20:48.000Z" title="发表于 2025-08-22 19:20:48">2025-08-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-31T12:26:40.995Z" title="更新于 2025-08-31 20:26:40">2025-08-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:30,&quot;messagePrev&quot;:&quot;自从上次更新，已经过了&quot;,&quot;messageNext&quot;:&quot;天，文章内容可能已经过时。&quot;,&quot;postUpdate&quot;:&quot;2025-08-31 20:26:40&quot;}" hidden></div><h1 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h1><p>Title: DINOv3</p><p>Author: <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Sim%C3%A9oni,+O">Oriane Siméoni</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Vo,+H+V">Huy V. Vo</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Seitzer,+M">Maximilian Seitzer</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Baldassarre,+F">Federico Baldassarre</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Oquab,+M">Maxime Oquab</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Jose,+C">Cijo Jose</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Khalidov,+V">Vasil Khalidov</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Szafraniec,+M">Marc Szafraniec</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Yi,+S">Seungeun Yi</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Ramamonjisoa,+M">Michaël Ramamonjisoa</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Massa,+F">Francisco Massa</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Haziza,+D">Daniel Haziza</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wehrstedt,+L">Luca Wehrstedt</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wang,+J">Jianyuan Wang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Darcet,+T">Timothée Darcet</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Moutakanni,+T">Théo Moutakanni</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Sentana,+L">Leonel Sentana</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Roberts,+C">Claire Roberts</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Vedaldi,+A">Andrea Vedaldi</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Tolan,+J">Jamie Tolan</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Brandt,+J">John Brandt</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Couprie,+C">Camille Couprie</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Mairal,+J">Julien Mairal</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=J%C3%A9gou,+H">Hervé Jégou</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Labatut,+P">Patrick Labatut</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Bojanowski,+P">Piotr Bojanowski</a></p><p>Publish: arxiv</p><p>Origanization: meta</p><p>Year: 2025</p><p>Code: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/dinov3">https://github.com/facebookresearch/dinov3</a></p><p>Dataset: LVD-1689M, SAT-493M</p><p>Keyword: 预训练，对比学习，自监督</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>自监督的好处：</p><ul><li>无需标注，可以使用近乎无限的数据</li><li>对分布偏移，提供全局和局部的特征，生成促进物理场景理解的丰富特征</li><li>无需人工干预，很适合用日益增长的网络数据长期学习</li></ul><blockquote><p>在视觉自监督中，优化方向有：</p><ul><li>基于重绘的方法：除了像素空间，可学习的的隐层空间预测(JEPA)</li><li>对比学习：主要从对比目标，信息论，自聚类策略</li></ul></blockquote><p>但仍有以下问题：</p><ul><li>模型不稳定和崩塌</li><li>尚不清楚如何从未标记的数据中找出有用的数据</li><li>使用cosine学习率的前提是知道一个优化边界作为先验</li><li>在早期学习后，性能反而会下降</li></ul><blockquote><p>在比ViT-Large更大的模型上跑更长时间的训练会出现</p></blockquote><p>目标从三个方面进行改进：</p><ul><li>通过scale模型尺寸和训练数据提供一个强大的通用模型</li><li>提升密集图特征(基于像素)，同时提供语义和底层几何特性</li></ul><blockquote><p>底层密集特征可以直接用于像素级任务，例如深度估计</p><blockquote><p>值得注意的是，高层语义理解可能会和底层密集特征质量冲突</p></blockquote><p>现有对比学习在相关方向上的改进：</p><ul><li>局部SSL损失</li><li>基于蒸馏和聚类的方法，使用多个encoder在不同层级的任务上训练</li><li>聚焦于事后的局部特征提升</li></ul></blockquote><ul><li>模型规模，使用7B训练，其它都是蒸馏</li></ul><p>贡献：</p><ul><li>数据scale方法</li><li>通过定义ViT的变种(使用现代的axial RoPE，正则化。。。)，常数超参数策略</li><li>使用了带有Gram anchoring训练阶段的pipeline，清楚特征图噪音，生成好的相似图，提升参数化和非参数化密集任务</li><li>将7B模型的能力迁移到一系列模型上</li></ul><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>从大模型的涌现现象得到灵感</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>数据的scale不仅在于数量，也在于质量(多样性，平衡，有用。。。)</p><p>DINOv2的数据量只有142M，Web-DINO的数据量在</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823221506387-20250831202614892.png" alt="image-20250823221506387"></p><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>在Instagram上的公开发表获取web图像，经过平台的过滤，有17B数据，创建三个数据部分：</p><ul><li>使用[基于多层级的K紧邻的自动curation方法](Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach)，使用DINOv2作为图像编码器，使用5层聚类(200M, 8M, 800K, 100K, 25K)，并使用平衡采样策略，得到1689M(LVD-1689M数据集)，创造一个包含所有网络图像的数据集</li><li>使用基于检索的系统，从数据池中选取与给定数据集相似的图像，创建一个与下游任务有关的数据集</li><li>使用ImageNet-1K，ImageNet-22K，Mapillary Street-level Sequences，通过DINOv2的方法优化最终性能</li></ul><h3 id="数据消融"><a href="#数据消融" class="headerlink" title="数据消融"></a>数据消融</h3><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823150017306-20250831202631602.png" alt="image-20250823150017306"></p><h2 id="自监督大规模预训练"><a href="#自监督大规模预训练" class="headerlink" title="自监督大规模预训练"></a>自监督大规模预训练</h2><blockquote><p>最近自监督发现scale up尽管在总体上表现良好，但在密集预测上性能差</p></blockquote><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><p>使用了多种全局和局部自监督目标的结合：</p><ul><li>图像级对比损失$L_{DINO}$</li><li>patch级重建损失$L_{iBOT}$</li></ul><blockquote><p>两点改进：</p><ul><li><p>两个损失中都用SwAV中的Sinkhorn-Knopp方法进行centering</p><blockquote><p>由于教师-学生网络，如果教师模型一直输出一个最大分布(所有都是一类)，就会使得模型训练崩塌，所以要将教师模型特征进行中心化，使其分布平衡</p></blockquote></li><li><p>同时在局部和全局的backbone输出后增加了一个专用的layer norm，稳定了ImageNet KNN分类的准确率，并提升了密集任务的性能</p></li></ul></blockquote><ul><li><p>使用一个Koleo正则化损失来让一个batch中的特征分布均匀(在一个batch的16个样本上应用)</p><blockquote><p>基于近邻距离，鼓励更大的近邻距离<br>$$<br>H \approx \frac{1}{N} \sum_{i&#x3D;1}^N \log d\left(x_i, x_i^{\mathrm{NN}}\right)<br>$$</p></blockquote></li></ul><p>总体为：<br>$$<br>\mathcal{L}<em>{\text {Pre }}&#x3D;\mathcal{L}</em>{\text {DINO }}+\mathcal{L}<em>{\text {iBOT }}+0.1 * \mathcal{L}</em>{\text {DKoleo }}<br>$$</p><h3 id="架构更新"><a href="#架构更新" class="headerlink" title="架构更新"></a>架构更新</h3><p>增加到7B参数，使用了变种的RoPE，原始的RoPE在[-1, 1]的内分配，为了使模型适用分辨率，缩放，纵横比，使用了RoPE-box变换，在[-s, s]的区间内进行分配，其中，$s\in[0.5, 2]$随机选择</p><blockquote><p>RoPE:</p><p>假定query和key之间的内积操作可以使用g表示：<br>$$<br>&lt;f_q(x_m), f_k(x_n)&gt;&#x3D;g(x_m, x_n)<br>$$<br>那么相对位置编码就是要找到：<br>$$<br>&lt;f_q(x_m, m), f_k(x_n, n)&gt;&#x3D;g(x_m, x_n, m-n)<br>$$<br>可以利用响亮的几何关系(或者说e指数相乘&#x3D;指数相加)得到如下关系：<br>$$<br>f_q(x_m, m) &#x3D; (W_qx_n)e^{in\theta}<br>$$</p><p>$$<br>f_w(x_m, n) &#x3D; (W_kx_n)e^{in\theta}<br>$$</p><p>$$<br>g(x_m, x_n, m-n) &#x3D; Re[(W_qx_m)(W_kx_n)*e^{i(m-n)\theta}]<br>$$</p><p>而$e^{i\theta}$可以使用欧拉公式分解，写为：<br>$$<br>\begin{aligned}<br>&amp; g\left(\boldsymbol{x}_m, \boldsymbol{x}_n, m-n\right) \<br>&amp; &#x3D;\left(\begin{array}{ll}<br>\boldsymbol{q}_m^{(1)} &amp; \boldsymbol{q}_m^{(2)}<br>\end{array}\right)\left(\begin{array}{cc}<br>\cos ((m-n) \theta) &amp; -\sin ((m-n) \theta) \<br>\sin ((m-n) \theta) &amp; \cos ((m-n) \theta)<br>\end{array}\right)\binom{k_n^{(1)}}{k_n^{(2)}}<br>\end{aligned}<br>$$</p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823155737518.png" alt="image-20250823155737518"></p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>由于数据规模和训练数据复杂度，很难估计一个先验，由此很难估计一个正确的优化边界，作者使用了一个常数学习率，权重衰减，教师模型EMA动量，由此，可以：</p><ul><li>只要下游任务有优化，就可以一直训练</li><li>减少超参数</li></ul><p>同时，对学习率和教师模型温度系数使用了线性warmup，使用AdamW，batch为4096(256GPU)，使用multi-crop策略(每张图片2个全局的crop和局部crop，每个crop为256(全局)&#x2F;112(局部)大小的正方形)</p><h2 id="Gram-Anchoring"><a href="#Gram-Anchoring" class="headerlink" title="Gram Anchoring"></a>Gram Anchoring</h2><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823164632783.png" alt="image-20250823164632783" style="zoom:50%"><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823164649622.png" alt="image-20250823164649622" style="zoom:50%"></p><p>可以看到，在以往的网络中，由于patch级的不一致性，像素级任务的性能会随着训练迭代而衰减</p><h3 id="patch级一致性损失"><a href="#patch级一致性损失" class="headerlink" title="patch级一致性损失"></a>patch级一致性损失</h3><p>对不同阶段patch和参考patch(红色patch)的余弦相似度进行了可视化，可以看到，在600k之后，出现了许多不相关的patch</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823165221252.png" alt="image-20250823165221252"></p><p>但是，CLS token和各个patch的相似度却在增加：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823165310782.png" alt="image-20250823165310782"></p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>可以从之前的实验看出，学习较强的判别行特征和保持局部一致性一定程度上不相互依赖</p><blockquote><p>可能会相互冲突？</p></blockquote><blockquote><p>尽管iBOT损失一定程度上增强了对于局部的学习，但是全局损失主导了整个训练过程</p></blockquote><p>由此，在不影响特征本身的情况下，可以强制要求patch级的一致性来解决</p><p>通过将学生模型的格拉姆矩阵推向之前的模型(称为格拉姆教师)来实现，选取一个早期iter的教师模型作为格拉姆教师</p><blockquote><p>格拉姆矩阵：两两向量的内积，反应各个向量间的关系<br>$$<br>\Delta\left(\alpha_1, \alpha_2, \ldots, \alpha_k\right)&#x3D;\left(\begin{array}{cccc}<br>\left(\alpha_1, \alpha_1\right) &amp; \left(\alpha_1, \alpha_2\right) &amp; \ldots &amp; \left(\alpha_1, \alpha_k\right) \<br>\left(\alpha_2, \alpha_1\right) &amp; \left(\alpha_2, \alpha_2\right) &amp; \ldots &amp; \left(\alpha_2, \alpha_k\right) \<br>\ldots &amp; \ldots &amp; \ldots &amp; \ldots \<br>\left(\alpha_k, \alpha_1\right) &amp; \left(\alpha_k, \alpha_2\right) &amp; \ldots &amp; \left(\alpha_k, \alpha_k\right)<br>\end{array}\right)<br>$$</p></blockquote><p>假设有P个patch的图像，网络维度为d，将网络的L2归一化后的局部特征表示为$X_S$，定义Gram损失为：<br>$$<br>\mathcal{L}_{\text {Gram }}&#x3D;\left|\mathbf{X}_S \cdot \mathbf{X}_S^{\top}-\mathbf{X}_G \cdot \mathbf{X}<em>G^{\top}\right|</em>{\mathrm{F}}^2<br>$$<br>只在全局crop上计算这个损失，并且只在1M iter之后计算，但这个损失仍能修复局部特征，同时，格拉姆教师模型在每10k iter更新一次</p><blockquote><p>如何只在全局crop上计算？</p></blockquote><p>由此，最终损失为：<br>$$<br>\mathcal{L}<em>{\text {Ref }}&#x3D;w</em>{\mathrm{D}} \mathcal{L}<em>{\text {DINO }}+\mathcal{L}</em>{\text {iBOT }}+w_{\text {DK }} \mathcal{L}<em>{\text {DKoleo }}+w</em>{\text {Gram }} \mathcal{L}_{\text {Gram }}<br>$$<br>从结果中可以看出，应用格拉姆损失之后，会影响iBOT的损失，但对DINO损失影响不大</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823180840513.png" alt="image-20250823180840513"></p><p>在下游的密集任务上，性能几乎立刻有了提升：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823180934501.png" alt="image-20250823180934501"></p><h3 id="高分辨率特征"><a href="#高分辨率特征" class="headerlink" title="高分辨率特征"></a>高分辨率特征</h3><p>最近研究发现对patch特征做一个加权平均可以提升局部特征(平滑外围patch并增强patch级的一致性)，另一方面，使用高分辨率图像可以生成更好和有更多细节的特征图</p><p>由此，对于格拉姆教师网络，使用两倍的分辨率输入图像，然后对于输出特征做两倍下采样，并利用双线性插值来获得更平滑的特征与学生网络的输出匹配</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823181931124.png" alt="image-20250823181931124"></p><blockquote><p>DINOv3可以处理任何分辨率图像</p></blockquote><p>用下采样后的特征取代之前损失中的$X_G$，形成新的优化目标$L_{HRef}$，由此可以让平滑的高分辨率特征蒸馏到学生网络中</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823182155912.png" alt="image-20250823182155912"></p><h2 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h2><h3 id="分辨率缩放"><a href="#分辨率缩放" class="headerlink" title="分辨率缩放"></a>分辨率缩放</h3><p>模型在256p分辨率上训练，patch为16，通过一个高分辨率适应阶段来拓展，使用混合分辨率，通过在batch中采样不同大小的全局{512, 768}和局部crop{112, 168, 224, 336}训练额外的10k iter</p><blockquote><p>与分辨率，patch14的DINOv2特征长度一致</p></blockquote><blockquote><p>有趣的是，最终模型能处理的最高分辨率超过了768，甚至可以达到4k</p></blockquote><h3 id="模型蒸馏"><a href="#模型蒸馏" class="headerlink" title="模型蒸馏"></a>模型蒸馏</h3><p>通过7B模型蒸馏得到ViT的变体，蒸馏方法使用与第一阶段训练相同的目标，使用7B模型(冻结)作为教师而非EMA，由于没有patch级一致性问题，也没有应用Gram anchoring</p><blockquote><p>在ViT边体重，增加了有额外参数的ViT-S+(29M)和ViT-H+(0.8B)</p></blockquote><h3 id="多学生蒸馏"><a href="#多学生蒸馏" class="headerlink" title="多学生蒸馏"></a>多学生蒸馏</h3><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823182921880.png" alt="image-20250823182921880"></p><h3 id="与文本对齐"><a href="#与文本对齐" class="headerlink" title="与文本对齐"></a>与文本对齐</h3><p>CLIP的对齐只针对于全局表征</p><blockquote><p>LiT: Zero-Shot Transfer with Locked-image text Tuning展现了预训练的视觉backbone可以实现有效的图像-文本对齐</p></blockquote><p>文中使用了DINOv2 meet text中的方法，大体是保持视觉encoder固定，训练一个文本模型来对齐图像的描述，为了一些灵活性，在视觉编码器后增加了两个transformer层，并且，使用平均池化的patch embedding和CLS token的拼接结果匹配文本embedding，可以同时实现全局和局部对齐</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>将DINOv3冻结</p><h2 id="密集特征"><a href="#密集特征" class="headerlink" title="密集特征"></a>密集特征</h2><h3 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h3><p>使用PCA将密集特征降维到3维，并映射到RGB</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823183800104.png" alt="image-20250823183800104"></p><h3 id="密集Linear-Probing"><a href="#密集Linear-Probing" class="headerlink" title="密集Linear Probing"></a>密集Linear Probing</h3><p>应用到语义分割和弹幕深度估计(都使用1024的patch长度，即$448\times448$使用patch 14，$512\times512$使用patch 16)</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823184113604.png" alt="image-20250823184113604"></p><h3 id="3D关系估计"><a href="#3D关系估计" class="headerlink" title="3D关系估计"></a>3D关系估计</h3><p>3D世界关系是基础模型重要的特征，测评了多视角一致性(一个物体不同视角的patch特征是否相似)，区分几何(相同物体)和语义(同类不同物体)估计</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823184353169.png" alt="image-20250823184353169"></p><h3 id="无监督物体发现"><a href="#无监督物体发现" class="headerlink" title="无监督物体发现"></a>无监督物体发现</h3><p>良好的自监督特征可以帮助在没有任何标注的情况下发现物体实例，通过使用非参数化基于图的TokenCut算法进行衡量</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823184709103.png" alt="image-20250823184709103"></p><blockquote><p>DINOv2尽管有很强的密集特征，但在物体发现上性能落后，可能由于密集特征中的artifacts特征</p></blockquote><h3 id="视频分割追踪"><a href="#视频分割追踪" class="headerlink" title="视频分割追踪"></a>视频分割追踪</h3><p>视觉特征的一个重要能力是时间一致性，在S(420&#x2F;480)，M(840&#x2F;960)，L(1260&#x2F;1140)分辨率上进行了实验</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823184921620.png" alt="image-20250823184921620"></p><p>尽管没有在视频上训练，还是取得了最好的性能</p><h3 id="视频分类"><a href="#视频分类" class="headerlink" title="视频分类"></a>视频分类</h3><p>进行高层视频语义分类，使用attentive probe(4层transformer分类器)</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823185643719.png" alt="image-20250823185643719"></p><h2 id="全局图像描述"><a href="#全局图像描述" class="headerlink" title="全局图像描述"></a>全局图像描述</h2><h3 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h3><p>在DINOv3的CLS token上训练一个线性分类器，带*的评估方法不同</p><p>对于ImageNet家族：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823185804649.png" alt="image-20250823185804649"></p><p>对于细粒度分类：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823185836527.png" alt="image-20250823185836527"></p><h3 id="实例识别"><a href="#实例识别" class="headerlink" title="实例识别"></a>实例识别</h3><p>使用一个非参数化搜索方法，使用CLS token，对数据库中的图像和给定图像做余弦相似度排序</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823190017055-20250831192129911.png" alt="image-20250823190017055"><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823190031762.png" alt="image-20250823190031762"></p><h2 id="复杂视觉系统"><a href="#复杂视觉系统" class="headerlink" title="复杂视觉系统"></a>复杂视觉系统</h2><h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h3><p>使用Plain-DETR(但不将trasformer encoder融入backbone中)，冻结backbone</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823190326741.png" alt="image-20250823190326741"></p><h3 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h3><p>使用ViT-Adapter和Mask2Former，但是，ViT-Adapter衣橱了injector部分，并且embedding dimension提升到了2048，与DINOv3的4096维度匹配，输入分辨率为896</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823191051866.png" alt="image-20250823191051866"></p><h3 id="单目深度估计"><a href="#单目深度估计" class="headerlink" title="单目深度估计"></a>单目深度估计</h3><p>使用Depth Anything V2，其使用了大量gt深度标注来合成图像</p><blockquote><p>这种合成-真实的gap，也是SAM无法处理的</p></blockquote><p>但把分辨率从$1024\times 768$提升到了DINOv3的高分辨率，同时，也scale了DPT头的大小</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192152593.png" alt="image-20250823192152593"></p><h3 id="视觉几何定位-3D理解"><a href="#视觉几何定位-3D理解" class="headerlink" title="视觉几何定位(3D理解)"></a>视觉几何定位(3D理解)</h3><p>使用VGGT，替换了其中的DINOv2(使用ViT-L)，输入分辨率为$518\times518$到$592\times592$</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192323989.png" alt="image-20250823192323989"></p><h2 id="整个DINOv3家族性能"><a href="#整个DINOv3家族性能" class="headerlink" title="整个DINOv3家族性能"></a>整个DINOv3家族性能</h2><p>蒸馏了ViT和ConvNext的变体</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192431396.png" alt="image-20250823192431396"></p><h3 id="ViT变种"><a href="#ViT变种" class="headerlink" title="ViT变种"></a>ViT变种</h3><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192524192.png" alt="image-20250823192524192"></p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192613894.png" alt="image-20250823192613894"></p><h3 id="ConvNext变种"><a href="#ConvNext变种" class="headerlink" title="ConvNext变种"></a>ConvNext变种</h3><blockquote><p>ConvNext计算量更小，并且卷积有更好的量化策略</p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192752037.png" alt="image-20250823192752037"></p><h3 id="零样本能力"><a href="#零样本能力" class="headerlink" title="零样本能力"></a>零样本能力</h3><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823192833106.png" alt="image-20250823192833106"></p><h1 id="遥感数据"><a href="#遥感数据" class="headerlink" title="遥感数据"></a>遥感数据</h1><p>在SAT-493M上进行训练，包含493M $512\times512$大小，0.6M分辨率，Maxar成像系统(包含Geoeye-1，WorldView-2等卫星)捕获的RGB图像</p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><h3 id="天空高度估计"><a href="#天空高度估计" class="headerlink" title="天空高度估计"></a>天空高度估计</h3><p>使用RGB-LiDAR数据，LiDAR数据作为高度的gt</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823193102969.png" alt="image-20250823193102969"></p><h3 id="地球观测"><a href="#地球观测" class="headerlink" title="地球观测"></a>地球观测</h3><p>很奇怪的是，Web图像还要比卫星图像更好，有几种可能的解释：</p><ul><li>SAT-493受限于成像系统，数据来源，并且只有0.6M分辨率，RGB</li><li>遥感图像和自然图像的领域差距并没有想象的那么大</li></ul><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250823193304355.png" alt="image-20250823193304355"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇文章探究了大规模自监督学习在数据处理，密集特征增强等方面：</p><ul><li><p>数据的scale不仅在于数量，也在于质量(多样性，平衡，噪声。。。)</p></li><li><p>语义特征和像素特征一定程度上不相互依赖，利用格拉姆矩阵损失提升密集特征</p></li><li><p>Scale并获得高质量的基础模型</p></li></ul><p>不足：</p><ul><li><p>遥感部分数据收集和结果异常</p></li><li><p>语义特征和密集特征不能解偶(数据量剧增，但分类，识别等任务提升微小)</p></li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top">Chanoch Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/">https://blog.chanoch.top/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.chanoch.top" target="_blank">Chanoch的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">自监督学习</a></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/05/07/uv%E5%BA%93/" title="uv库使用"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">uv库使用</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/08/31/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv2/" title="论文笔记-DINOv2"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-31</div><div class="info-item-2">论文笔记-DINOv2</div></div><div class="info-2"><div class="info-item-1">信息Title: DINOv2: Learning Robust Visual Features without Supervision Author: Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Chanoch Li</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/chanochLi" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:kujou@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">信息</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">3.1.</span> <span class="toc-text">数据准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">数据收集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B6%88%E8%9E%8D"><span class="toc-number">3.1.2.</span> <span class="toc-text">数据消融</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.2.</span> <span class="toc-text">自监督大规模预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">3.2.1.</span> <span class="toc-text">学习目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E6%9B%B4%E6%96%B0"><span class="toc-number">3.2.2.</span> <span class="toc-text">架构更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-number">3.2.3.</span> <span class="toc-text">优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gram-Anchoring"><span class="toc-number">3.3.</span> <span class="toc-text">Gram Anchoring</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#patch%E7%BA%A7%E4%B8%80%E8%87%B4%E6%80%A7%E6%8D%9F%E5%A4%B1"><span class="toc-number">3.3.1.</span> <span class="toc-text">patch级一致性损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.2.</span> <span class="toc-text">目标函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E7%89%B9%E5%BE%81"><span class="toc-number">3.3.3.</span> <span class="toc-text">高分辨率特征</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8E%E8%AE%AD%E7%BB%83"><span class="toc-number">3.4.</span> <span class="toc-text">后训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%BE%A8%E7%8E%87%E7%BC%A9%E6%94%BE"><span class="toc-number">3.4.1.</span> <span class="toc-text">分辨率缩放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F"><span class="toc-number">3.4.2.</span> <span class="toc-text">模型蒸馏</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%AD%A6%E7%94%9F%E8%92%B8%E9%A6%8F"><span class="toc-number">3.4.3.</span> <span class="toc-text">多学生蒸馏</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%96%87%E6%9C%AC%E5%AF%B9%E9%BD%90"><span class="toc-number">3.4.4.</span> <span class="toc-text">与文本对齐</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%86%E9%9B%86%E7%89%B9%E5%BE%81"><span class="toc-number">4.1.</span> <span class="toc-text">密集特征</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E5%88%86%E6%9E%90"><span class="toc-number">4.1.1.</span> <span class="toc-text">定性分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%86%E9%9B%86Linear-Probing"><span class="toc-number">4.1.2.</span> <span class="toc-text">密集Linear Probing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3D%E5%85%B3%E7%B3%BB%E4%BC%B0%E8%AE%A1"><span class="toc-number">4.1.3.</span> <span class="toc-text">3D关系估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E7%89%A9%E4%BD%93%E5%8F%91%E7%8E%B0"><span class="toc-number">4.1.4.</span> <span class="toc-text">无监督物体发现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E9%A2%91%E5%88%86%E5%89%B2%E8%BF%BD%E8%B8%AA"><span class="toc-number">4.1.5.</span> <span class="toc-text">视频分割追踪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB"><span class="toc-number">4.1.6.</span> <span class="toc-text">视频分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0"><span class="toc-number">4.2.</span> <span class="toc-text">全局图像描述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">4.2.1.</span> <span class="toc-text">图像分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E8%AF%86%E5%88%AB"><span class="toc-number">4.2.2.</span> <span class="toc-text">实例识别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E8%A7%86%E8%A7%89%E7%B3%BB%E7%BB%9F"><span class="toc-number">4.3.</span> <span class="toc-text">复杂视觉系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">4.3.1.</span> <span class="toc-text">目标检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-number">4.3.2.</span> <span class="toc-text">语义分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E7%9B%AE%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="toc-number">4.3.3.</span> <span class="toc-text">单目深度估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E5%87%A0%E4%BD%95%E5%AE%9A%E4%BD%8D-3D%E7%90%86%E8%A7%A3"><span class="toc-number">4.3.4.</span> <span class="toc-text">视觉几何定位(3D理解)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%B8%AADINOv3%E5%AE%B6%E6%97%8F%E6%80%A7%E8%83%BD"><span class="toc-number">4.4.</span> <span class="toc-text">整个DINOv3家族性能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ViT%E5%8F%98%E7%A7%8D"><span class="toc-number">4.4.1.</span> <span class="toc-text">ViT变种</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvNext%E5%8F%98%E7%A7%8D"><span class="toc-number">4.4.2.</span> <span class="toc-text">ConvNext变种</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%B6%E6%A0%B7%E6%9C%AC%E8%83%BD%E5%8A%9B"><span class="toc-number">4.4.3.</span> <span class="toc-text">零样本能力</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">5.</span> <span class="toc-text">遥感数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="toc-number">5.1.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A9%E7%A9%BA%E9%AB%98%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="toc-number">5.1.1.</span> <span class="toc-text">天空高度估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%B0%E7%90%83%E8%A7%82%E6%B5%8B"><span class="toc-number">5.1.2.</span> <span class="toc-text">地球观测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/" title="[论文笔记] DINOv3">[论文笔记] DINOv3</a><time datetime="2025-08-22T11:20:48.000Z" title="发表于 2025-08-22 19:20:48">2025-08-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/07/uv%E5%BA%93/" title="uv库使用">uv库使用</a><time datetime="2025-05-07T04:22:39.000Z" title="发表于 2025-05-07 12:22:39">2025-05-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/04/Jetson%E6%95%99%E7%A8%8B/" title="Jetson+gemini 2相机配置教程">Jetson+gemini 2相机配置教程</a><time datetime="2025-05-04T04:22:39.000Z" title="发表于 2025-05-04 12:22:39">2025-05-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/31/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv2/" title="论文笔记-DINOv2">论文笔记-DINOv2</a><time datetime="2024-08-31T12:25:14.000Z" title="发表于 2024-08-31 20:25:14">2024-08-31</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"><div id="footer-wrap"><div class="copyright">&copy;2025 By Chanoch Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'all',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
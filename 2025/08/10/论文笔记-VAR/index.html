<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>[论文笔记] VAR | Chanoch的博客</title><meta name="author" content="Chanoch Li"><meta name="copyright" content="Chanoch Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="信息Title: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction Author: Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang Publish: NeurIPS Year: 2024 Keyword: Aut"><meta property="og:type" content="article"><meta property="og:title" content="[论文笔记] VAR"><meta property="og:url" content="https://blog.chanoch.top/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/index.html"><meta property="og:site_name" content="Chanoch的博客"><meta property="og:description" content="信息Title: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction Author: Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang Publish: NeurIPS Year: 2024 Keyword: Aut"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog.chanoch.top/img/avatar.png"><meta property="article:published_time" content="2025-08-10T11:20:48.000Z"><meta property="article:modified_time" content="2025-09-04T06:03:03.150Z"><meta property="article:author" content="Chanoch Li"><meta property="article:tag" content="自回归"><meta property="article:tag" content="图像生成"><meta property="article:tag" content="多尺度"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.chanoch.top/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[论文笔记] VAR",
  "url": "https://blog.chanoch.top/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/",
  "image": "https://blog.chanoch.top/img/avatar.png",
  "datePublished": "2025-08-10T11:20:48.000Z",
  "dateModified": "2025-09-04T06:03:03.150Z",
  "author": [
    {
      "@type": "Person",
      "name": "Chanoch Li",
      "url": "https://blog.chanoch.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="https://blog.chanoch.top/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"[论文笔记] VAR",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Chanoch的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">[论文笔记] VAR</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">[论文笔记] VAR</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-10T11:20:48.000Z" title="发表于 2025-08-10 19:20:48">2025-08-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-04T06:03:03.150Z" title="更新于 2025-09-04 14:03:03">2025-09-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>8分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:30,&quot;messagePrev&quot;:&quot;自从上次更新，已经过了&quot;,&quot;messageNext&quot;:&quot;天，文章内容可能已经过时。&quot;,&quot;postUpdate&quot;:&quot;2025-09-04 14:03:03&quot;}" hidden></div><h2 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h2><p>Title: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</p><p>Author: <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Tian,+K">Keyu Tian</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Jiang,+Y">Yi Jiang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Yuan,+Z">Zehuan Yuan</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Peng,+B">Bingyue Peng</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wang,+L">Liwei Wang</a></p><p>Publish: NeurIPS</p><p>Year: 2024</p><p>Keyword: Autoregressive, self-supervised</p><p>Code: <a target="_blank" rel="noopener" href="https://github.com/FoundationVision/VAR">https://github.com/FoundationVision/VAR</a></p><p>关键词：自回归，Transformer，图像生成</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>NLP中，LLM在泛化性和多样性上朝AGI更进一步，其核心在于自回归的自监督学习模型的可拓展性和泛化性(scaling law和零样本能力)</p><p>CV中，就尝试利用序列编码器将图像patch变为2Dtoken，并变为1D token做自回归，但拓展后性能不好，落后于扩散模型</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/Screenshot%202025-01-12%20at%2010.10.48.png" alt="Screenshot 2025-01-12 at 10.10.48"></p><blockquote><p>Frechet Inception distance是衡量生成模型生成质量的参数，衡量了整个数据集中生成图像和原始图像之间分布的距离（即一个数据集计算一个值）</p><p>计算中，首先两张图片先经过一个网络（一般为Inception v3），其符合高斯分布，然后对特征计算均值和方差，最终计算距离：<br>$$<br>\mathrm{FID}&#x3D;\left|\mu_r-\mu_g\right|^2+\operatorname{Tr}\left(\Sigma_r+\Sigma_g-2\left(\Sigma_r \Sigma_g\right)^{1 &#x2F; 2}\right)<br>$$</p><blockquote><p>值得注意的是，FID是针对于整个数据集的指标，而非像LPIPS一样针对单张图像</p></blockquote><p>Inception Score (IS)用于衡量生成图像的质量，主要标准为现实性和多样性，使用Inception v3分类模型衡量，现实性体现在能生成能使模型准确分类的图像，多样从模型能生成一系列类的图像而非一部分类</p></blockquote><p>视觉基础模型：CLIP，SAM，Dinov2，提示工程：Painter，LVM</p><p>Raster扫描自回归：VQVAE，VQGAN，VQVAE-2，RQ-Transformer，Parti</p><p>掩码预测：MaskGIT，MagViT，MUSE</p><p>扩散模型：DiT，U-ViT，Stable diffusion，SORA，Vidu</p><p>自回归要点在于数据的顺序，人类一般多层次接受，首先看整体结构，然后逐渐关注细节信息</p><p>作者将视觉的自回归定义为下一个尺度的预测，Visual Autoregressive modeling</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241223145057623.png" alt="image-20241223145057623"></p><p>贡献：</p><ul><li>提出了多尺度预测的自回归生成方式，由整体结构到细节信息进行预测</li><li>使用类似GPT-2的结构，实现多尺度视觉自回归</li><li>验证了模型的可拓展性和零样本能力</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h4 id="前置信息：基于下个token预测的自回归"><a href="#前置信息：基于下个token预测的自回归" class="headerlink" title="前置信息：基于下个token预测的自回归"></a>前置信息：基于下个token预测的自回归</h4><p>基于单项序列依赖的假设，序列$x$的可能性就可以因式分解：<br>$$<br>p\left(x_1, x_2, \ldots, x_T\right)&#x3D;\prod_{t&#x3D;1}^T p\left(x_t \mid x_1, x_2, \ldots, x_{t-1}\right)<br>$$<br>图像是2D连续信号，需要进行序列化，并决定顺序：<br>$$<br>f&#x3D;\mathcal{E}(i m), \quad q&#x3D;\mathcal{Q}(f)<br>$$<br>其中，im是原始图像，$\mathcal{E}(\cdot)$是编码器，$\mathcal{Q}(\cdot)$是序列化器，序列化器通常包含一个词典，会将特征向量映射到欧几里得空间中的编码序列：<br>$$<br>q^{(i, j)}&#x3D;\left(\underset{v \in[V]}{\arg \min }\left|\operatorname{lookup}(Z, v)-f^{(i, j)}\right|<em>2\right) \in[V]<br>$$<br>由此，通过词书$Z$得到一个编号，并用decoder获取重建的图像，并计算损失：<br>$$<br>\begin{aligned}<br>&amp; \hat{f}&#x3D;\operatorname{lookup}(Z, q), \quad i \hat{m}&#x3D;\mathcal{D}(\hat{f}) \<br>&amp; \mathcal{L}&#x3D;|i m-i \hat{m}|<em>2+|f-\hat{f}|<em>2+\lambda</em>{\mathrm{P}} \mathcal{L}</em>{\mathrm{P}}(i \hat{m})+\lambda</em>{\mathrm{G}} \mathcal{L}<em>{\mathrm{G}}(i \hat{m})<br>\end{aligned}<br>$$<br>其中，$\mathcal{L}</em>{\mathrm{P}}(\cdot)$是一个感性的损失，例如LPIPS，$\mathcal{L}_{\mathrm{G}}(\cdot)$是一个判别行损失</p><blockquote><p>更像是encoder-映射-decoder来进行自回归</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904135225801.png" alt="image-20250904135225801"></p></blockquote><p>由此，自编码器${\mathcal{E}, \mathcal{Q}, \mathcal{D}}$就可以进行单向自回归</p><p>缺点：</p><ul><li><p>假设违背：这样产生的所有特征$f^{(i, j)}$，相互之间存在依赖，flatten之后就会有双向依赖，但对于自回归，建立在应该只有单向依赖上</p><blockquote><p>VQVAE的注意力分数图(1D token)</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241225163240461.png" alt="image-20241225163240461"></p></blockquote></li><li><p>无法适用于下游任务：需要相互关系的下游任务无法应用</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904135350915.png" alt="image-20250904135350915" style="zoom:50%"><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904135400357.png" alt="image-20250904135400357" style="zoom:50%"></p></li><li><p>结构退化：图像空间结构也包含有信息</p></li><li><p>效率低：transformer是$\mathcal{O}\left(n^2\right)$，加上自回归就是$\mathcal{O}\left(n^6\right)$</p><blockquote><p>时间复杂度分析：</p><p>引理：对于标准的self-attention，对于AR的复杂度为$\mathcal{O}\left(n^6\right)$，其中$h&#x3D;w&#x3D;n$</p><p>证明：</p><p>所有token的数量是$n^2$</p><blockquote><p>但值得注意的是，这是基于像素的回归，而不是基于patch的回归</p></blockquote><p>由此需要回归$n^2$次，而transformer的计算复杂度为$\mathcal{O}\left(i^2\right)$</p><p>由此可以求得总的时间复杂度为：<br>$$<br>\sum_{i&#x3D;1}^{n^2} i^2&#x3D;\frac{1}{6} n^2\left(n^2+1\right)\left(2 n^2+1\right)<br>$$<br>即$\mathcal{O}\left(n^6\right)$</p></blockquote></li></ul><blockquote><p>但需要注意：</p><p>1.此处是基于像素的回归，n代表像素数量，但在基于视觉的模型中较少使用，给出的对比方法中也是21，22年左右的方法</p><p>2.时间复杂度并不代表实际时间，只是时间随分辨率增长的趋势</p></blockquote><h4 id="基于下一尺度预测的视觉自回归"><a href="#基于下一尺度预测的视觉自回归" class="headerlink" title="基于下一尺度预测的视觉自回归"></a>基于下一尺度预测的视觉自回归</h4><p>首先将一张图量化为K个多尺度的$\left(r_1, r_2, \ldots, r_K\right)$，分辨率依次递增，那么自回归就变成了：<br>$$<br>p\left(r_1, r_2, \ldots, r_K\right)&#x3D;\prod_{k&#x3D;1}^K p\left(r_k \mid r_1, r_2, \ldots, r_{k-1}\right)<br>$$<br>使用了block因果mask来保证只看到之前的token：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241226161502682.png" alt="image-20241226161502682"></p><p>量化：采用了VQGAN中的结构，但修改了其中的多尺度量化层，使用了同样的词本：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241226162410572.png" alt="image-20241226162410572"></p><p>重建的时候也用了类似VQGAN的结构：</p><p><strong><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241226163440251.png" alt="image-20241226163440251"></strong></p><p>为了解决上采样的信息损失问题，额外使用了K个卷积层</p><p>其复杂度为：$\mathcal{O}\left(n^4\right)$</p><p>假设：</p><ul><li>h&#x3D;w&#x3D;n为VQVAE获取的最大的特征图的分辨率（实际为原图分辨率）</li><li>•生成的不同分辨率的图像的宽和高为$a^{(K-1)}&#x3D;n$，其中$\alpha$为缩放系数</li></ul><p>最终需要计算自回归的token为$\left(r_1, r_2, \ldots, r_k\right)$，一共需要计算k次，由于分辨率依次递增，每次的计算量不同：<br>$$<br>\sum_{i&#x3D;1}^k n_i^2&#x3D;\sum_{i&#x3D;1}^k a^{2 \cdot(k-1)}&#x3D;\frac{a^{2 k}-1}{a^2-1}<br>$$<br>最终加和得到：<br>$$<br>\begin{aligned}<br>&amp; \sum_{k&#x3D;1}^{\log _a(n)+1}\left(\frac{a^{2 k}-1}{a^2-1}\right)^2 \<br>&amp; &#x3D;\frac{\left(a^4-1\right) \log n+\left(a^8 n^4-2 a^6 n^2-2 a^4\left(n^2-1\right)+2 a^2-1\right) \log a}{\left(a^2-1\right)^3\left(a^2+1\right) \log a} \<br>&amp; \sim \mathcal{O}\left(n^4\right)<br>\end{aligned}<br>$$</p><h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><p>量化器：词本所有尺度为$V&#x3D;4096$，量化器在OpenImages上进行训练，下采样率为$16\times$</p><p>Transformer：</p><ul><li><p>decoder-only的transformer</p></li><li><p>使用adaptive normalization(AdaLN)</p></li><li><p>对于特定类的合成，使用了class embedding作为初始token和AdaLN的条件</p></li><li><p>在attention归一化query和key可以稳定训练，使用了类relu的归一化：<br>$$<br>w&#x3D;64 d, \quad h&#x3D;d, \quad d r&#x3D;0.1 \cdot d &#x2F; 24<br>$$</p><p>其中，$w$是宽度，$h$是head的数量，$dr$是drop比率</p></li></ul><p>由此，总参数量为：<br>$$<br>N(d)&#x3D;\underbrace{d \cdot 4 w^2}<em>{\text {self-attention }}+\underbrace{d \cdot 8 w^2}</em>{\text {feed-forward }}+\underbrace{d \cdot 6 w^2}_{\text {adaptive layernorm }}&#x3D;18 d w^2&#x3D;73728 d^3<br>$$</p><p>超参数：</p><p>$lr&#x3D;10^{-4}$ per 256 batch，batch&#x3D;256，AdamW(${\beta}_1&#x3D;0.9, {\beta}_2&#x3D;0.95, decay&#x3D;0.05$)，batch&#x3D;768<del>1024，epoch&#x3D;200</del>350</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h4 id="对比试验"><a href="#对比试验" class="headerlink" title="对比试验"></a>对比试验</h4><p>对于$256\times256$，带有类别的生成</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241226165900687.png" alt="image-20241226165900687"></p><p>对于$512\times512$的图像：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241227103354643.png" alt="image-20241227103354643"></p><h4 id="scaling能力"><a href="#scaling能力" class="headerlink" title="scaling能力"></a>scaling能力</h4><p>之前的研究认为扩展AR模型会导致测试损失有一个可预期的下降：<br>$$<br>L&#x3D;(\beta \cdot X)^\alpha<br>$$<br>其中，$X$可以任意为参数量$N$，训练token $T$，优化训练计算量$C_{min}$，指数上标$\alpha$表明power-law的平滑性，$L$代表被不可削减的损失$L_{\infty}$归一化的可削减损失<br>$$<br>\log (L)&#x3D;\alpha \log (X)+\alpha \log \beta<br>$$<br>测试了$18M$到$2B$的模型，包含1.28M图像或者870B的token</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>方法优势：</p><ul><li><p>利用自回归的尺度预测方式，可以把多尺度特征的优势引入图像生成，自监督预训练和下游任务</p></li><li><p>多尺度的自回归预测，在一定程度上解决了图像自回归训练的因果依赖问题，实现了合理的视觉自回归</p></li></ul><p>可能的劣势：</p><ul><li><p>时间复杂度的计算问题：基本单位和结构设计并不一致，放在一起比较并没有实际意义，实际上，其训练速度很慢</p></li><li><p>对于多尺度的特征生成的方式还可能进行改进，一方面VQGAN的各层特征并不一定具有物理意义，另一方面，不同分辨率的特征纬度是不统一的</p></li></ul><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904135531801.png" alt="image-20250904135531801"></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top">Chanoch Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/">https://blog.chanoch.top/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.chanoch.top" target="_blank">Chanoch的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E5%9B%9E%E5%BD%92/">自回归</a><a class="post-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a><a class="post-meta__tags" href="/tags/%E5%A4%9A%E5%B0%BA%E5%BA%A6/">多尺度</a></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/" title="[论文笔记] DINOv3"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">[论文笔记] DINOv3</div></div><div class="info-2"><div class="info-item-1">信息Title: DINOv3 Author: Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan...</div></div></div></a><a class="pagination-related" href="/2025/07/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dove/" title="[论文笔记] dove"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">[论文笔记] dove</div></div><div class="info-2"><div class="info-item-1">信息Title: Images are Worth Variable Length of Representations Author: Lingjun Mao, Rodolfo Corona, Xin Liang, Wenhao Yan, Zineng Tang Year: 2025 Publish: arxiv Organization: University of California, University of Washington Code:...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Chanoch Li</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/chanochLi" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:kujou@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%EF%BC%9A%E5%9F%BA%E4%BA%8E%E4%B8%8B%E4%B8%AAtoken%E9%A2%84%E6%B5%8B%E7%9A%84%E8%87%AA%E5%9B%9E%E5%BD%92"><span class="toc-number">3.0.1.</span> <span class="toc-text">前置信息：基于下个token预测的自回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%B8%8B%E4%B8%80%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B%E7%9A%84%E8%A7%86%E8%A7%89%E8%87%AA%E5%9B%9E%E5%BD%92"><span class="toc-number">3.0.2.</span> <span class="toc-text">基于下一尺度预测的视觉自回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">3.0.3.</span> <span class="toc-text">实现细节</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E8%AF%95%E9%AA%8C"><span class="toc-number">4.0.1.</span> <span class="toc-text">对比试验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#scaling%E8%83%BD%E5%8A%9B"><span class="toc-number">4.0.2.</span> <span class="toc-text">scaling能力</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number"></span> <span class="toc-text">总结</span></a></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/" title="[论文笔记] DINOv3">[论文笔记] DINOv3</a><time datetime="2025-08-24T11:20:48.000Z" title="发表于 2025-08-24 19:20:48">2025-08-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/" title="[论文笔记] VAR">[论文笔记] VAR</a><time datetime="2025-08-10T11:20:48.000Z" title="发表于 2025-08-10 19:20:48">2025-08-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dove/" title="[论文笔记] dove">[论文笔记] dove</a><time datetime="2025-07-27T12:59:29.000Z" title="发表于 2025-07-27 20:59:29">2025-07-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BitNetV2/" title="[论文笔记] BitNetV2">[论文笔记] BitNetV2</a><time datetime="2025-05-25T12:25:14.000Z" title="发表于 2025-05-25 20:25:14">2025-05-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BitNet/" title="[论文笔记] BitNet">[论文笔记] BitNet</a><time datetime="2025-05-24T12:25:14.000Z" title="发表于 2025-05-24 20:25:14">2025-05-24</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"><div id="footer-wrap"><div class="copyright">&copy;2025 By Chanoch Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'all',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>[论文笔记] ARM | Chanoch的博客</title><meta name="author" content="Chanoch Li"><meta name="copyright" content="Chanoch Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="信息Title: Autoregressive Pretraining with Mamba in Vision Author: Sucheng Ren, Xianhang Li, Haoqin Tu, Feng Wang, Fangxun Shu, Lei Zhang, Jieru Mei, Linjie Yang, Peng Wang, Heng Wang, Alan Yuille, Ciha"><meta property="og:type" content="article"><meta property="og:title" content="[论文笔记] ARM"><meta property="og:url" content="https://blog.chanoch.top/2024/12/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ARM/index.html"><meta property="og:site_name" content="Chanoch的博客"><meta property="og:description" content="信息Title: Autoregressive Pretraining with Mamba in Vision Author: Sucheng Ren, Xianhang Li, Haoqin Tu, Feng Wang, Fangxun Shu, Lei Zhang, Jieru Mei, Linjie Yang, Peng Wang, Heng Wang, Alan Yuille, Ciha"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog.chanoch.top/img/avatar.png"><meta property="article:published_time" content="2024-12-08T12:25:14.000Z"><meta property="article:modified_time" content="2025-09-04T06:36:36.450Z"><meta property="article:author" content="Chanoch Li"><meta property="article:tag" content="自回归"><meta property="article:tag" content="Mamba"><meta property="article:tag" content="视觉特征"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.chanoch.top/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[论文笔记] ARM",
  "url": "https://blog.chanoch.top/2024/12/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ARM/",
  "image": "https://blog.chanoch.top/img/avatar.png",
  "datePublished": "2024-12-08T12:25:14.000Z",
  "dateModified": "2025-09-04T06:36:36.450Z",
  "author": [
    {
      "@type": "Person",
      "name": "Chanoch Li",
      "url": "https://blog.chanoch.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="https://blog.chanoch.top/2024/12/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ARM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"[论文笔记] ARM",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Chanoch的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">[论文笔记] ARM</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">[论文笔记] ARM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-08T12:25:14.000Z" title="发表于 2024-12-08 20:25:14">2024-12-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-04T06:36:36.450Z" title="更新于 2025-09-04 14:36:36">2025-09-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>8分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:30,&quot;messagePrev&quot;:&quot;自从上次更新，已经过了&quot;,&quot;messageNext&quot;:&quot;天，文章内容可能已经过时。&quot;,&quot;postUpdate&quot;:&quot;2025-09-04 14:36:36&quot;}" hidden></div><h1 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h1><p>Title: Autoregressive Pretraining with Mamba in Vision</p><p>Author: <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Ren,+S">Sucheng Ren</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Li,+X">Xianhang Li</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Tu,+H">Haoqin Tu</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wang,+F">Feng Wang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Shu,+F">Fangxun Shu</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Zhang,+L">Lei Zhang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Mei,+J">Jieru Mei</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Yang,+L">Linjie Yang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wang,+P">Peng Wang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wang,+H">Heng Wang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Yuille,+A">Alan Yuille</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Xie,+C">Cihang Xie</a></p><p>Year: 2024</p><p>Publish: arxiv</p><p>Code: <a target="_blank" rel="noopener" href="https://github.com/OliverRensu/ARM">https://github.com/OliverRensu/ARM</a></p><p>Keyword: Mamba, 自回归, 扫描策略</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>SSM在NLP和视觉领域由于可以用线性复杂度处理长序列关系，Mamba利用选择扫描机制进行了实现</p><blockquote><p>经典SSM模型：<br>$$<br>\begin{aligned}<br>h^{\prime}(t) &amp; &#x3D;\mathbf{A} h(t)+\mathbf{B} x(t) \<br>y(t) &amp; &#x3D;\mathbf{C} h(t)<br>\end{aligned}<br>$$<br>零阶保持离散化：<br>$$<br>\begin{aligned}<br>&amp; \overline{\mathbf{A}}&#x3D;\exp (\boldsymbol{\Delta} \mathbf{A}) \<br>&amp; \overline{\mathbf{B}}&#x3D;(\boldsymbol{\Delta} \mathbf{A})^{-1}(\exp (\boldsymbol{\Delta} \mathbf{A})-\mathbf{I}) \cdot \boldsymbol{\Delta} \mathbf{B}<br>\end{aligned}<br>$$<br>定义快速计算：<br>$$<br>\begin{aligned}<br>\overline{\mathbf{K}} &amp; &#x3D;\left(\mathbf{C} \overline{\mathbf{B}}, \mathbf{C} \overline{\mathbf{A B}}, \ldots, \mathbf{C} \overline{\mathbf{A}}^k \overline{\mathbf{B}}, \ldots\right) \<br>\mathbf{y} &amp; &#x3D;\mathbf{x} * \overline{\mathbf{K}}<br>\end{aligned}<br>$$</p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904141444730.png" alt="image-20250904141444730"></p><p>但之前主要集中于有监督学习（泛化性和scaling）</p><blockquote><p>相较于其它方法，自回归</p></blockquote><p>SSM：</p><ul><li><p>没有Softmax的线性attention可以视为一种退化的SSM</p></li><li><p>S5在S4的基础上增添了多入多出SSM和并行扫描策略</p></li><li><p>RWKV是一种类似于像有两个SSM一样运行的RNN，融合了状态拓展和输入无关的门控机制</p></li><li><p>Mamba是使用了隐藏状态拓展的数据无关的SSM层</p></li></ul><p>自回归：</p><ul><li><p>iGPT第一个在视觉领域使用了自回归</p></li><li><p>SAIM和RandSAC继续提升了自回归，使用了ViT和随机序列排列</p></li><li><p>D-iGPT在自回归时不仅预测下一个token，并且预测可见token</p></li><li><p>AIM进行scale</p></li></ul><p>Vision Mamba：</p><ul><li>Vim：Vim层使用了堆叠的纯Mamba层，使用了双向扫描</li><li>Vmamba：使用了VSS层，结合了Mamba层和2D卷积，使用了与类似Swin的金字塔结构，每个VSS层通过2D深度卷积-CrossScan进行处理</li><li>Mamba-ND：增强Mamba到多维度</li><li>LocalMamba：对图像切分，然后对于这些window使用SSM</li><li>EfficientMamba使用空洞采样来减少计算量</li></ul><h3 id="Mamba背景"><a href="#Mamba背景" class="headerlink" title="Mamba背景"></a>Mamba背景</h3><p>状态空间模型使用拓展的隐藏状态$h_t$对将1D的序列进行建模，其中，隐藏状态通过参数A，B，C随着时间变化，变化符合线性常微分方程：<br>$$<br>\begin{aligned}<br>h^{\prime}(t) &amp; &#x3D;\mathbf{A} h(t)+\mathbf{B} x(t) \<br>y(t) &amp; &#x3D;\mathbf{C} h(t)<br>\end{aligned}<br>$$<br>引入一个时间缩放参数$\Delta$，使用零阶保持来对连续的A，B进行离散化：</p><blockquote><p>对第一个线性常微分方程两边同乘$e^{-A t}$可以得到：<br>$$<br>e^{-A t} h^{\prime}(t)&#x3D;e^{-A t} A h(t)+e^{-A t} B x(t)<br>$$<br>移项化简：<br>$$<br>\frac{d}{d t}\left(e^{-A t} h(t)\right)&#x3D;e^{-A t} B x(t)<br>$$<br>同时取积分：<br>$$<br>e^{-A t} h(t)-h(t_0)&#x3D;\int_{t_0}^t e^{-A \tau} B x(\tau) d \tau<br>$$<br>左乘$e^{A t}$并移项得$h(t)$：<br>$$<br>h(t)&#x3D;e^{A t} h(t_0)+\int_{t_0}^t e^{A(t-\tau)} B x(\tau) d \tau<br>$$<br>进行离散化，代入$k&#x3D;(k+1)T$：<br>$$<br>h((k+1) T)&#x3D;e^{A(k+1) T} h(kT)+\int_{kT}^{(k+1) T} e^{A((k+1) T-\tau)} B x(\tau) d \tau<br>$$<br><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241209141426870.png" alt="image-20241209141426870"></p><p>零阶保持的假设是：<br>$$<br>x(t)&#x3D;x_k, \quad t \in\left[kT, (k+1)T \right),<br>$$<br>那么可以令$\alpha&#x3D;k T+T-\tau$对上式进行化简：<br>$$<br>h((k+1) T)&#x3D;e^{A T} h(k T)+\left(\int_0^T e^{A \alpha} d \alpha\right) B x(k T)<br>$$</p></blockquote><p>$$<br>\begin{aligned}<br>&amp; \overline{\mathbf{A}}&#x3D;\exp (\boldsymbol{\Delta} \mathbf{A}) \<br>&amp; \overline{\mathbf{B}}&#x3D;(\boldsymbol{\Delta} \mathbf{A})^{-1}(\exp (\boldsymbol{\Delta} \mathbf{A})-\mathbf{I}) \cdot \boldsymbol{\Delta} \mathbf{B}<br>\end{aligned}<br>$$</p><p>最终变为：<br>$$<br>\begin{aligned}<br>h^{\prime}(t) &amp; &#x3D;\overline{\mathbf{A}} h_{t-1}+\overline{\mathbf{B}} x_t, \<br>y_t &amp; &#x3D;\mathbf{C} h_t .<br>\end{aligned}<br>$$</p><blockquote><p>$$<br>h_t&#x3D;\bar{A} h_{t-1}+\bar{B} x_t<br>$$</p><p>$$<br>h_{t-1}&#x3D;\bar{A} h_{t-2}+\bar{B} x_{t-1}<br>$$</p><p>$$<br>h_t&#x3D;\bar{A}^2 h_{t-2}+\bar{A} \bar{B} x_{t-1}+\bar{B} x_t&#x3D;\sum_{k&#x3D;0}^t \bar{A}^k \bar{B} x_{t-k}<br>$$</p><p>$$<br>y_t&#x3D;C h_t&#x3D;C\left(\sum_{k&#x3D;0}^t \bar{A}^k \bar{B} x_{t-k}\right)&#x3D;\sum_{k&#x3D;0}^t C \bar{A}^k \bar{B} x_{t-k}<br>$$</p><p>而离散卷积：<br>$$<br>(x * h)[t]&#x3D;\sum_{k&#x3D;0}^{N-1} x[k] h[t-k]<br>$$</p></blockquote><p>定义一个矩阵$\overline{\mathbf{K}}$进行快速计算：<br>$$<br>\begin{aligned}<br>\overline{\mathbf{K}} &amp; &#x3D;\left(\mathbf{C} \overline{\mathbf{B}}, \mathbf{C} \overline{\mathbf{A B}}, \ldots, \mathbf{C} \overline{\mathbf{A}}^k \overline{\mathbf{B}}, \ldots\right) \<br>\mathbf{y} &amp; &#x3D;\mathbf{x} * \overline{\mathbf{K}}<br>\end{aligned}<br>$$<br>其中，$k \in[0, L)$，$L$为输入序列长度，$\overline{\mathbf{K}} \in \mathbb{R}^L$可以被视为卷积核，这可以使得Mamba像自回归一样对输入序列建模</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241209142109688.png" alt="image-20241209142109688"></p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241209141553788.png" alt="image-20241209141553788"></p><p>贡献点（对于输入特征进行改进）：</p><ul><li>将自回归预训练与Mamba模型结合，探索视觉Mamba架构的自监督范式</li><li>利用聚类将临近的patch聚为簇作为输入，替代基于像素或patch的策略</li><li>对于将2D图像映射为1D输入，证明逐行扫描已经足够有效</li></ul><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h3 id="视觉中自监督"><a href="#视觉中自监督" class="headerlink" title="视觉中自监督"></a>视觉中自监督</h3><p>图像和文本信息存在本质上的差异，没有很强的因果关系：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904141838487.png" alt="image-20250904141838487" style="zoom:67%"><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20250904141844344.png" alt="image-20250904141844344" style="zoom:67%"></p><ul><li>文本的自监督研究主要集中于自回归，可以做到scaling law</li><li>图像的自监督研究可以分为对比学习，MAE，自回归，位置预测，Jigsaw…，对于哪种方法更有效和如何scale模型还没有统一的结论，主要难点在于：<ul><li>实现有效且稳定的自监督学习（模型，代理任务…）</li><li>预训练方法在所有下游任务上都有效（对比偏好分类，MAE偏好生成任务…）</li><li>如何简单有效地拓展模型（ViT大规模自监督训练易崩溃，Mamba拓展性问题）</li></ul></li></ul><h3 id="自回归预训练"><a href="#自回归预训练" class="headerlink" title="自回归预训练"></a>自回归预训练</h3><h5 id="NLP中的自回归预训练"><a href="#NLP中的自回归预训练" class="headerlink" title="NLP中的自回归预训练"></a>NLP中的自回归预训练</h5><p>NLP中的自回归是对语料库（句子）$\mathcal{U}&#x3D;\left{u_1, \ldots, u_n\right}$中下一个单词的概率的预测：<br>$$<br>p(u)&#x3D;\prod_{i&#x3D;1}^n p\left(u_i \mid u_1, \ldots, u_{i-1}, \Theta\right)<br>$$<br>通过最小化负对数概率（最大化预测出句子的概率）：<br>$$<br>\mathcal{L}&#x3D;-\log p(u)<br>$$</p><h5 id="视觉中带Mamba的自回归预训练"><a href="#视觉中带Mamba的自回归预训练" class="headerlink" title="视觉中带Mamba的自回归预训练"></a>视觉中带Mamba的自回归预训练</h5><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241205091733676.png" alt="image-20241205091733676"></p><p>预测单位：</p><ul><li>基于像素：首先需要定义2D中自回归的预测单位，iGPT定义为像素</li></ul><p>$$<br>\begin{aligned}<br>&amp; \mathcal{L}&#x3D;\sum_{i&#x3D;1}^{n-1} l\left(f\left(\left[p_1, \ldots, p_i\right]\right), p_{i+1}\right) \<br>&amp; l(\hat{y}, y)&#x3D;|\hat{y}-y|^2<br>\end{aligned}<br>$$</p><p>​	其中，$f(\cdot)$代表模型</p><p>​	这种基于像素的方法，引入了大量计算，在iGPT中，就使用了低分辨率的图像</p><ul><li><p>基于patch：将图像切分为不重叠的patch，并转换为视觉token<br>$$<br>\begin{aligned}<br>&amp; \mathcal{L}&#x3D;\sum_{i&#x3D;1}^{n-1} l\left(f\left(\left[P_1, \ldots, P_i\right]\right), P_{i+1}\right), \<br>&amp; l(\hat{y}, y)&#x3D;|\hat{y}-y|^2 .<br>\end{aligned}<br>$$</p></li><li><p>基于patch簇的：将空间上邻近的patch组成更长的token<br>$$<br>\begin{aligned}<br>\mathcal{L}<em>{\mathrm{ARM}} &amp; &#x3D;\sum</em>{i&#x3D;1}^{n-1} l\left(f\left(\left[c_1, \ldots, c_i\right]\right), c_{i+1}\right) \<br>l(\hat{y}, y) &amp; &#x3D;|\hat{y}-y|^2<br>\end{aligned}<br>$$</p></li></ul><p>预测顺序：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241205093353346.png" alt="image-20241205093353346"></p><blockquote><p>对于将2D图像变换为1D序列后如何决定预测顺序，但更多是以实验为主，缺乏相关的理论依据</p></blockquote><h3 id="MambaMLP"><a href="#MambaMLP" class="headerlink" title="MambaMLP"></a>MambaMLP</h3><p>由Transformer Block启发而来，在预训练和微调阶段不同，将Mamba块用作于token混合，MLP用作于通道混合：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241205093818317.png" alt="image-20241205093818317"></p><p>在预训练阶段，只有一次扫描来符合自回归的单方向，在微调阶段，使用4个方向的扫描，类似于VMamba中的双向扫描来获取全局信息</p><p>扫描的expand参数设定为1，Vim使用expand参数为2</p><blockquote><p>expand参数越大，性能更好，但推理速度更慢</p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241205135542556.png" alt="image-20241205135542556"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h3 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h3><p>在ImageNet-1k上预训练，ARM-B和ARM-L为1600epoch，ARM-H为800epoch，batch分别为2048，1024，512，lr为$1.5 \mathrm{e}-4 \times \frac{\text { batchsize }}{256}$，余弦退火策略，warm-up为5epoch，AdamW优化器输入为$192\times192$，使用随机裁剪和flipping</p><p>在ImageNet分类上进行微调，100epoch，batch1024，输入为$224\times224$，增强与MAE相同，AdamW，lr&#x3D;$5 \mathrm{e}-4 \times \frac{\text { batchsize }}{256}$，余弦退火策略，warm-up为5epoch，使用了EMA</p><p>在ImageNet的变体上进行了测试，包括自然增强（ImageNet-A），语义变化（ImageNet-R），骨架（ImageNet-S），ImageNet-V2，ImageNet-Real</p><h3 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h3><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241205152105443.png" alt="image-20241205152105443"></p><blockquote><p>其它模型没有采用了自监督预训练</p></blockquote><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241206110132230.png" alt="image-20241206110132230"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>簇中patch的消融：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241206111232929.png" alt="image-20241206111232929"></p><p>预测顺序：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241206111709438.png" alt="image-20241206111709438"></p><p>Decoder设计：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241206143648716.png" alt="image-20241206143648716"></p><p>目标设计：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241206143701376.png" alt="image-20241206143701376"></p><p>不同的代理任务：</p><p><img src="https://raw.githubusercontent.com/chanochLi/blog-pic/main/img/image-20241206143818309.png" alt="image-20241206143818309"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>文章探索了自回归预训练与Mamba模型结合的训练，探索视觉Mamba架构的自监督范式，然而：</p><ul><li>视觉自回归方法缺乏理论基础或者直觉经验</li><li>性能和拓展性仍比不上最新的ViT结构</li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top">Chanoch Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.chanoch.top/2024/12/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ARM/">https://blog.chanoch.top/2024/12/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ARM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.chanoch.top" target="_blank">Chanoch的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E5%9B%9E%E5%BD%92/">自回归</a><a class="post-meta__tags" href="/tags/Mamba/">Mamba</a><a class="post-meta__tags" href="/tags/%E8%A7%86%E8%A7%89%E7%89%B9%E5%BE%81/">视觉特征</a></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-webssl/" title="[论文笔记] Web-SSL"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">[论文笔记] Web-SSL</div></div><div class="info-2"><div class="info-item-1">信息Title: Scaling Language-Free Visual Representation Learning Author: David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, Saining Xie Year: 2025 Publish:...</div></div></div></a><a class="pagination-related" href="/2024/08/31/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv2/" title="[论文笔记] DINOv2"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">[论文笔记] DINOv2</div></div><div class="info-2"><div class="info-item-1">信息Title: DINOv2: Learning Robust Visual Features without Supervision Author: Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/" title="[论文笔记] VAR"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-10</div><div class="info-item-2">[论文笔记] VAR</div></div><div class="info-2"><div class="info-item-1">信息Title: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction Author: Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang Publish: NeurIPS Year: 2024 Keyword: Autoregressive, self-supervised Code:...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Chanoch Li</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/chanochLi" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:kujou@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">信息</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">背景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mamba%E8%83%8C%E6%99%AF"><span class="toc-number">2.0.1.</span> <span class="toc-text">Mamba背景</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E4%B8%AD%E8%87%AA%E7%9B%91%E7%9D%A3"><span class="toc-number">3.0.1.</span> <span class="toc-text">视觉中自监督</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.0.2.</span> <span class="toc-text">自回归预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#NLP%E4%B8%AD%E7%9A%84%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.0.2.0.1.</span> <span class="toc-text">NLP中的自回归预训练</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E4%B8%AD%E5%B8%A6Mamba%E7%9A%84%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.0.2.0.2.</span> <span class="toc-text">视觉中带Mamba的自回归预训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MambaMLP"><span class="toc-number">3.0.3.</span> <span class="toc-text">MambaMLP</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%AE%9A"><span class="toc-number">4.0.1.</span> <span class="toc-text">实验设定</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.0.2.</span> <span class="toc-text">对比实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.0.3.</span> <span class="toc-text">消融实验</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-DINOv3/" title="[论文笔记] DINOv3">[论文笔记] DINOv3</a><time datetime="2025-08-24T11:20:48.000Z" title="发表于 2025-08-24 19:20:48">2025-08-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-VAR/" title="[论文笔记] VAR">[论文笔记] VAR</a><time datetime="2025-08-10T11:20:48.000Z" title="发表于 2025-08-10 19:20:48">2025-08-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dove/" title="[论文笔记] dove">[论文笔记] dove</a><time datetime="2025-07-27T12:59:29.000Z" title="发表于 2025-07-27 20:59:29">2025-07-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BitNetV2/" title="[论文笔记] BitNetV2">[论文笔记] BitNetV2</a><time datetime="2025-05-25T12:25:14.000Z" title="发表于 2025-05-25 20:25:14">2025-05-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BitNet/" title="[论文笔记] BitNet">[论文笔记] BitNet</a><time datetime="2025-05-24T12:25:14.000Z" title="发表于 2025-05-24 20:25:14">2025-05-24</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"><div id="footer-wrap"><div class="copyright">&copy;2025 By Chanoch Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'all',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>